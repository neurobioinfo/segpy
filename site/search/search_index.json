{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Segpy's documentation! Segpy is a comprehensive pipeline to facilitate variant segregation analysis. Segregation analysis investigates how genetic variants are passed down through generations in a family in order to identify genetic traits that contribute to disease. In general, segregation analysis comprises a three-step process: Family pedigree construction : A detailed family tree is created to visualize relationships and inheritance patterns among family members. Genotyping : Family members are genotyped to identify the presence or absence of specific variants. Data Analysis : Statistical methods are applied to analyze how the variants segregate with the phenotype within the family. This often involves comparing the genotype of affected individuals to that of unaffected individuals. While Segpy was originally designed for segregation analysis, it can also be used to analyze case-control cohorts . We provide comprehensive tutorials for each application case in the Tutorial section of this documentation. Regardless of the application case, Segpy computes the counts of affected and non-affected individuals, both in and out of families, based on reference allele, alternate allele, homozygous alternate allele, and no variant call. These counts are assembled into a comprehensive dataframe, where each row represents a single variant, which can be optionally annotated with Variant Effect Predictor (VEP) to facilitate downstream statistical analyses. The Segpy pipeline comprises four sequential steps shown in Figure 1 . As input, users must provide a VCF file, which can be optionally annotated with VEP, and a pedigree file. The VCF file is then converted to the Hail MatrixTable format, the variants are segregated, and the counts data frame is parsed to only retain informative ouputs. Figure 1. Segpy pipeline workflow. A containerized version of the Segpy pipeline is publicly available from Zenodo , which includes the code, libraries, and dependicies required for running the analyses. Segpy is configured to seamlessly integrate with High-Performance Computing (HPC) systems utilizing the SLURM workload manager (Segpy SLURM), but can also be run locally on linux workstations (Segpy Local). In this documentation, we provide a step-by-step tutorial and explanation of the Segpy pipeline. We also provide a quick-start tutorial using a simulated dataset. Contents Tutorial: Installation Segpy SLURM Segpy Local Configuration parameters Output files FAQ About: License Changelog Contributing Acknowledgement","title":"Home"},{"location":"#welcome-to-segpys-documentation","text":"Segpy is a comprehensive pipeline to facilitate variant segregation analysis. Segregation analysis investigates how genetic variants are passed down through generations in a family in order to identify genetic traits that contribute to disease. In general, segregation analysis comprises a three-step process: Family pedigree construction : A detailed family tree is created to visualize relationships and inheritance patterns among family members. Genotyping : Family members are genotyped to identify the presence or absence of specific variants. Data Analysis : Statistical methods are applied to analyze how the variants segregate with the phenotype within the family. This often involves comparing the genotype of affected individuals to that of unaffected individuals. While Segpy was originally designed for segregation analysis, it can also be used to analyze case-control cohorts . We provide comprehensive tutorials for each application case in the Tutorial section of this documentation. Regardless of the application case, Segpy computes the counts of affected and non-affected individuals, both in and out of families, based on reference allele, alternate allele, homozygous alternate allele, and no variant call. These counts are assembled into a comprehensive dataframe, where each row represents a single variant, which can be optionally annotated with Variant Effect Predictor (VEP) to facilitate downstream statistical analyses. The Segpy pipeline comprises four sequential steps shown in Figure 1 . As input, users must provide a VCF file, which can be optionally annotated with VEP, and a pedigree file. The VCF file is then converted to the Hail MatrixTable format, the variants are segregated, and the counts data frame is parsed to only retain informative ouputs. Figure 1. Segpy pipeline workflow. A containerized version of the Segpy pipeline is publicly available from Zenodo , which includes the code, libraries, and dependicies required for running the analyses. Segpy is configured to seamlessly integrate with High-Performance Computing (HPC) systems utilizing the SLURM workload manager (Segpy SLURM), but can also be run locally on linux workstations (Segpy Local). In this documentation, we provide a step-by-step tutorial and explanation of the Segpy pipeline. We also provide a quick-start tutorial using a simulated dataset.","title":"Welcome to Segpy's documentation!"},{"location":"#contents","text":"Tutorial: Installation Segpy SLURM Segpy Local Configuration parameters Output files FAQ About: License Changelog Contributing Acknowledgement","title":"Contents"},{"location":"Acknowledgement/","text":"Acknowledgement The pipeline is done as part of MNI projects, it is written by Saeid Amiri with associate with Dan Spiegelman, and Sali Farhan at Neuro Bioinformatics Core. Copyright belongs MNI BIOINFO CORE .","title":"Acknowledgement"},{"location":"Acknowledgement/#acknowledgement","text":"The pipeline is done as part of MNI projects, it is written by Saeid Amiri with associate with Dan Spiegelman, and Sali Farhan at Neuro Bioinformatics Core. Copyright belongs MNI BIOINFO CORE .","title":"Acknowledgement"},{"location":"FAQ/","text":"Frequently asked questions What is finalseg.csv? What is finalseg.csv? The output of finalseg.csv is the unpruned result from step 2.","title":"FAQ"},{"location":"FAQ/#frequently-asked-questions","text":"What is finalseg.csv?","title":"Frequently asked questions"},{"location":"FAQ/#what-is-finalsegcsv","text":"The output of finalseg.csv is the unpruned result from step 2.","title":"What is finalseg.csv?"},{"location":"LICENSE/","text":"MIT License Copyright (c) 2022 The Neuro Bioinformatics Core Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/","text":"Coming soon","title":"Coming soon"},{"location":"about/#coming-soon","text":"","title":"Coming soon"},{"location":"changelog/","text":"Changelog v0.0.1 This is the initial release. v0.0.2 Supposed to Complete the documentation","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v001","text":"This is the initial release.","title":"v0.0.1"},{"location":"changelog/#v002","text":"Supposed to Complete the documentation","title":"v0.0.2"},{"location":"contributing/","text":"Contributing This is an initial version, and any contributions or suggestions are welcomed. The pipeline is designed to be user-friendly for segregation analysis. For direct contact, please reach out to The Neuro Bioinformatics Core via [neurobioinfo@mcgill.ca]. If you encounter any issue , please report them on the GitHub repository.","title":"Contributing"},{"location":"contributing/#contributing","text":"This is an initial version, and any contributions or suggestions are welcomed. The pipeline is designed to be user-friendly for segregation analysis. For direct contact, please reach out to The Neuro Bioinformatics Core via [neurobioinfo@mcgill.ca]. If you encounter any issue , please report them on the GitHub repository.","title":"Contributing"},{"location":"installation/","text":"Installation A containerized version of the Segpy pipeline is publicly available from Zenodo , which includes the code, libraries, and dependicies required for running the analyses. The container is compatible with both High-Performance Computing (HPC) systems and standard Linux workstations. To use the Segpy pipeline, the folowing must be installed on your system: Apptainer segpy.pip Apptainer segpy.pip is packaged and tested with Apptainer (formerly Singularity) version 1.2.4. Before proceeding, ensure that Apptainer/Singularity is installed on your system. If you are using an HPC system, Apptainer is likely already installed, and you will simply need to load the module. If you encounter any issues while loading the Apptainer module, please contact your system administrator. Before running the Segpy pipeline, load the Apptainer module using the following command: # Load Apptainer module apptainer/1.2.4 Segpy.pip To download the latest version of Segpy run the following command: # Download the Segpy container #curl \"https://zenodo.org/records/12751010/files/scrnabox.slurm.zip?download=1\" --output segpy.pip.zip # Unzip the Segpy container unzip segpy.pip.zip To ensure that segpy.pip is installed properly, run the following command: bash /path/to/segpy.pip/launch_segpy.sh -h If segpy.pip is installed properly, the above command should return the folllowing: ------------------------------------ segregation pipeline version 0.0.3 is loaded ------------------- Usage: /home/sam/seg_cont/segpy003/segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -V (--verbose) = verbose output After successfully installing segpy.pip we can proceed with the segregation analysis. \u2b06 back to top","title":"Installation"},{"location":"installation/#installation","text":"A containerized version of the Segpy pipeline is publicly available from Zenodo , which includes the code, libraries, and dependicies required for running the analyses. The container is compatible with both High-Performance Computing (HPC) systems and standard Linux workstations. To use the Segpy pipeline, the folowing must be installed on your system: Apptainer segpy.pip","title":"Installation"},{"location":"installation/#apptainer","text":"segpy.pip is packaged and tested with Apptainer (formerly Singularity) version 1.2.4. Before proceeding, ensure that Apptainer/Singularity is installed on your system. If you are using an HPC system, Apptainer is likely already installed, and you will simply need to load the module. If you encounter any issues while loading the Apptainer module, please contact your system administrator. Before running the Segpy pipeline, load the Apptainer module using the following command: # Load Apptainer module apptainer/1.2.4","title":"Apptainer"},{"location":"installation/#segpypip","text":"To download the latest version of Segpy run the following command: # Download the Segpy container #curl \"https://zenodo.org/records/12751010/files/scrnabox.slurm.zip?download=1\" --output segpy.pip.zip # Unzip the Segpy container unzip segpy.pip.zip To ensure that segpy.pip is installed properly, run the following command: bash /path/to/segpy.pip/launch_segpy.sh -h If segpy.pip is installed properly, the above command should return the folllowing: ------------------------------------ segregation pipeline version 0.0.3 is loaded ------------------- Usage: /home/sam/seg_cont/segpy003/segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -V (--verbose) = verbose output After successfully installing segpy.pip we can proceed with the segregation analysis. \u2b06 back to top","title":"Segpy.pip"},{"location":"output/","text":"Segpy outputs The Segpy pipeline computes the counts of affected and non-affected individuals, both in and out of families, based on reference allele, alternate allele, homozygous alternate allele, and no variant call. These counts are assembled into a comprehensive dataframe, where each row represents a single variant, which can be optionally annotated with Variant Effect Predictor (VEP) to facilitate downstream statistical analyses. Step 3 of the Segpy pipeline produces two output files, depending on the parsing parameter used: finalseg_cleaned_general.csv : produced using the --parser general tag to remove uncessary characters, such as \" , [, ] , etc. finalseg_cleaned_unique.csv : produced using the --parser unique tag to eliminate duplicated variant entries resulting from VEP annotations. Regardless of the parsing parameters used, the output file includes separate columns for the user-selected VEP annotations and distinct columns detailing the variant counts across individuals in the study, categorized by their disease and family status: Column title Description Family - Affected fam_aff_wild Family - Affecteds: wildtype fam_aff_ncl Family - Affecteds: no call fam_aff_vrt Family - Affecteds: with variant fam_aff_homv Family - Affecteds: homozygous for ALT allele Family - Non-affected fam_naf_wild Family - non-affecteds: wildtype fam_naf_ncl Family - non-affecteds: no call fam_naf_vrt Family - non-affecteds: with variant fam_naf_homv Family - non-affecteds: homozygous for ALT allele Non-Family - Affected nfm_aff_wild Non-family - Affecteds: wildtype nfm_aff_ncl Non-family - Affecteds: no call nfm_aff_vrt Non-family - Affecteds: with variant nfm_aff_homv Non-family - Affecteds: homozygous for ALT allele Non-Family - Non-affected nfm_naf_wild Non-family - non-affecteds: wildtype nfm_naf_ncl Non-family - non-affecteds: no call nfm_naf_vrt Non-family - non-affecteds: with variant nfm_naf_homv Non-family - non-affecteds: homozygous for ALT allele","title":"Output files"},{"location":"output/#segpy-outputs","text":"The Segpy pipeline computes the counts of affected and non-affected individuals, both in and out of families, based on reference allele, alternate allele, homozygous alternate allele, and no variant call. These counts are assembled into a comprehensive dataframe, where each row represents a single variant, which can be optionally annotated with Variant Effect Predictor (VEP) to facilitate downstream statistical analyses. Step 3 of the Segpy pipeline produces two output files, depending on the parsing parameter used: finalseg_cleaned_general.csv : produced using the --parser general tag to remove uncessary characters, such as \" , [, ] , etc. finalseg_cleaned_unique.csv : produced using the --parser unique tag to eliminate duplicated variant entries resulting from VEP annotations. Regardless of the parsing parameters used, the output file includes separate columns for the user-selected VEP annotations and distinct columns detailing the variant counts across individuals in the study, categorized by their disease and family status: Column title Description Family - Affected fam_aff_wild Family - Affecteds: wildtype fam_aff_ncl Family - Affecteds: no call fam_aff_vrt Family - Affecteds: with variant fam_aff_homv Family - Affecteds: homozygous for ALT allele Family - Non-affected fam_naf_wild Family - non-affecteds: wildtype fam_naf_ncl Family - non-affecteds: no call fam_naf_vrt Family - non-affecteds: with variant fam_naf_homv Family - non-affecteds: homozygous for ALT allele Non-Family - Affected nfm_aff_wild Non-family - Affecteds: wildtype nfm_aff_ncl Non-family - Affecteds: no call nfm_aff_vrt Non-family - Affecteds: with variant nfm_aff_homv Non-family - Affecteds: homozygous for ALT allele Non-Family - Non-affected nfm_naf_wild Non-family - non-affecteds: wildtype nfm_naf_ncl Non-family - non-affecteds: no call nfm_naf_vrt Non-family - non-affecteds: with variant nfm_naf_homv Non-family - non-affecteds: homozygous for ALT allele","title":"Segpy outputs"},{"location":"reference/","text":"Segpy configuration parameters The configs directory produced after initiating the Segpy pipeline (step 0) contains the segpy.config.ini , which contains execution parameters that users must modify prior to running their analysis. The segpy.config.ini can be modified using the following command: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x The following parameters are adjustable for the Segpy pipeline: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' The version of Singularity/Apptainer loaded onto your system. ACCOUNT ACCOUNT=user Your SLURM user account. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 Number of CPUs for the step 1 job sumission. THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 Amount of memory (RAM) for the step 1 job sumission. MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g Amount of time for the step 1 job sumission. WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 Number of CPUs for the step 2 job sumission. THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 Amount of memory (RAM) for the step 2 job sumission. MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g Amount of time for the step 2 job sumission. WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 Number of CPUs for the step 3 job sumission. THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 Amount of memory (RAM) for the step 3 job sumission. MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g Amount of time for the step 3 job sumission.","title":"Configuration parameters"},{"location":"reference/#segpy-configuration-parameters","text":"The configs directory produced after initiating the Segpy pipeline (step 0) contains the segpy.config.ini , which contains execution parameters that users must modify prior to running their analysis. The segpy.config.ini can be modified using the following command: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x The following parameters are adjustable for the Segpy pipeline: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' The version of Singularity/Apptainer loaded onto your system. ACCOUNT ACCOUNT=user Your SLURM user account. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 Number of CPUs for the step 1 job sumission. THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 Amount of memory (RAM) for the step 1 job sumission. MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g Amount of time for the step 1 job sumission. WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 Number of CPUs for the step 2 job sumission. THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 Amount of memory (RAM) for the step 2 job sumission. MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g Amount of time for the step 2 job sumission. WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 Number of CPUs for the step 3 job sumission. THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 Amount of memory (RAM) for the step 3 job sumission. MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g Amount of time for the step 3 job sumission.","title":"Segpy configuration parameters"},{"location":"segpy_local/","text":"Segpy Local In this tutorial we illustrate how to run the Segpy pipeline on a local Linux workstation. Contents Input data Step 0: Setup Step 1: VCF to MatrixTable Step 2: Run segregation Step 3: Parse output file The following flowchart illustrates the steps for running the segregation analysis on a local Linux workstation. Figure 1. Segpy pipeline workflow. Input data To execute the pipeline, users must provide two separate input files: VCF Pedigree file The VCF should be formatted according to the standard VCF specifications, containing information about genetic variants, including their positions, alleles, and genotype information for each individual in the study. The Pedigree file should be in .ped format, structured such that each line describes an individual in the study and includes the following columns: familyid , individualid , parentalid , maternalid , sex , phenotype , facilitating the analysis of inheritance patterns and genetic associations within the family. The familyid column must contain identifiers for the family. The individualid column must contain identifiers for the individual that match the VCF file. The parentalid column must contain identifiers for the father (0 if unknown). The maternalid column must contain identifiers for the mother (0 if unknown). The sex column must describe the biological sex of the individual (1 = male, 2 = female, 0 = unknown). The phenotype column must describe the phenotypic data (1 = unaffected, 2 = affected, -9 = missing). We provide an example .ped file HERE . Step 0: Setup Prior to initiating the pipeline, users must provide the paths to: The directory containing segpy.pip (PIPELINE_HOME) The directory designated for the pipeline's outputs (PWD) The VCF file (VCF) The pedigree file (PED) Use the following code to define these paths: # Define necessary paths export PIPELINE_HOME=path/to/segpy.pip PWD=path/to/outfolder VCF=path/to/VCF.vcf PED=path/to/pedigree.ped To ensure that segpy.pip was defined properly, run the following command: bash $PIPELINE_HOME/launch_segpy.sh -h Which should return the following: ------------------------------------ segregation pipeline version 0.0.3 is loaded ------------------- Usage: /home/sam/seg_cont/segpy003/segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -V (--verbose) = verbose output Once the necessary paths have been defined, we can initialize the pipeline using the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 0 \\ After running this command, the working directory (PWD) should have the following structure: PWD \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 segpy.config.ini \u251c\u2500\u2500 launch_summary_log.txt \u2514\u2500\u2500 logs \u251c\u2500\u2500 jobs \u2514\u2500\u2500 spark The configs directory contains a .ini file with adjustable parameters for the pipeline. Please see HEREEEEEEEEEEEEEEEEE for more information regarding these parameters. The logs directory documents the parameters and outputs from each analytical step of the Segpy pipeline to ensure reproducibility. The launch_summary_log.txt is a cumulative documentation of all submitted jobs throughout the analysis. After completing Step 0, open the segpy.config.ini file to adjust the general parameters for the analysis. The following parameters must be adjusted: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE=NA The version of Singularity/Apptainer loaded onto your system. For Local workstations, you can keep this parameter commented. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Step 1: VCF to MatrixTable In step 1, we will convert the user-provided VCF file to the Hail MatrixTable format, which is designed to efficiently store and manipulate large-scale genomic datasets. To run step 1 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1 \\ --vcf $VCF After running this command, a step1 directory will be created in the working directory (PWD), which will contain the output files from Hail. The step1 directory should have the following structure: step1 \u2514\u2500\u2500 VEP_iPSC.mt \u251c\u2500\u2500 cols \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 entries \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 globals \u2502 \u251c\u2500\u2500 globals \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545.idx \u2502 \u251c\u2500\u2500 index \u2502 \u2514\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 README.txt \u251c\u2500\u2500 references \u251c\u2500\u2500 rows \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u2514\u2500\u2500 _SUCCESS A deatiled description of the outputs can be found in the Hail documentation Step 2: Run segregation In step 2, we will leverage the Hail ouputs from step 1 to perform segregation analysis. To run step 2 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --vcf $VCF --ped $PED After running this command, a step2 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step2 directory should have the following structure: step2 \u251c\u2500\u2500 finalseg.csv \u2514\u2500\u2500 temp The finalseg.csv file contains the variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see HEREEEEEEEEEEEEEEEEEEE for a comprehensive description of the file contents. Step 3: Parse output file In step 3, we will parse the finalseg.csv obtained from step 2 to simplify the outputs. If you simply want to remove unnecessary characters such as \" , [, ] , etc., use the following command to run step 3: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser general If you want to eliminate duplicated variant entries resulting from VEP annotations, use the following command to run step 3: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser unique After running this command, a step3 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step3 directory should have the following structure: step3 \u2514\u2500\u2500 finalseg_cleaned_general.csv \u2514\u2500\u2500 finalseg_cleaned_unique.csv The finalseg_cleaned_general.csv and finalseg_cleaned_unique.csv files contain the parsed variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see HEREEEEEEEEEEEEEEEEEEE for a comprehensive description of the file contents. Note : You can execute steps 1 to 3 sequentially, using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1-3 \\ --vcf $VCF \\ --ped $PED \\ --parser general \u2b06 back to top","title":"Segpy Local"},{"location":"segpy_local/#segpy-local","text":"In this tutorial we illustrate how to run the Segpy pipeline on a local Linux workstation.","title":"Segpy Local"},{"location":"segpy_local/#contents","text":"Input data Step 0: Setup Step 1: VCF to MatrixTable Step 2: Run segregation Step 3: Parse output file The following flowchart illustrates the steps for running the segregation analysis on a local Linux workstation. Figure 1. Segpy pipeline workflow.","title":"Contents"},{"location":"segpy_local/#input-data","text":"To execute the pipeline, users must provide two separate input files: VCF Pedigree file The VCF should be formatted according to the standard VCF specifications, containing information about genetic variants, including their positions, alleles, and genotype information for each individual in the study. The Pedigree file should be in .ped format, structured such that each line describes an individual in the study and includes the following columns: familyid , individualid , parentalid , maternalid , sex , phenotype , facilitating the analysis of inheritance patterns and genetic associations within the family. The familyid column must contain identifiers for the family. The individualid column must contain identifiers for the individual that match the VCF file. The parentalid column must contain identifiers for the father (0 if unknown). The maternalid column must contain identifiers for the mother (0 if unknown). The sex column must describe the biological sex of the individual (1 = male, 2 = female, 0 = unknown). The phenotype column must describe the phenotypic data (1 = unaffected, 2 = affected, -9 = missing). We provide an example .ped file HERE .","title":"Input data"},{"location":"segpy_local/#step-0-setup","text":"Prior to initiating the pipeline, users must provide the paths to: The directory containing segpy.pip (PIPELINE_HOME) The directory designated for the pipeline's outputs (PWD) The VCF file (VCF) The pedigree file (PED) Use the following code to define these paths: # Define necessary paths export PIPELINE_HOME=path/to/segpy.pip PWD=path/to/outfolder VCF=path/to/VCF.vcf PED=path/to/pedigree.ped To ensure that segpy.pip was defined properly, run the following command: bash $PIPELINE_HOME/launch_segpy.sh -h Which should return the following: ------------------------------------ segregation pipeline version 0.0.3 is loaded ------------------- Usage: /home/sam/seg_cont/segpy003/segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -V (--verbose) = verbose output Once the necessary paths have been defined, we can initialize the pipeline using the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 0 \\ After running this command, the working directory (PWD) should have the following structure: PWD \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 segpy.config.ini \u251c\u2500\u2500 launch_summary_log.txt \u2514\u2500\u2500 logs \u251c\u2500\u2500 jobs \u2514\u2500\u2500 spark The configs directory contains a .ini file with adjustable parameters for the pipeline. Please see HEREEEEEEEEEEEEEEEEE for more information regarding these parameters. The logs directory documents the parameters and outputs from each analytical step of the Segpy pipeline to ensure reproducibility. The launch_summary_log.txt is a cumulative documentation of all submitted jobs throughout the analysis. After completing Step 0, open the segpy.config.ini file to adjust the general parameters for the analysis. The following parameters must be adjusted: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE=NA The version of Singularity/Apptainer loaded onto your system. For Local workstations, you can keep this parameter commented. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x","title":"Step 0: Setup"},{"location":"segpy_local/#step-1-vcf-to-matrixtable","text":"In step 1, we will convert the user-provided VCF file to the Hail MatrixTable format, which is designed to efficiently store and manipulate large-scale genomic datasets. To run step 1 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1 \\ --vcf $VCF After running this command, a step1 directory will be created in the working directory (PWD), which will contain the output files from Hail. The step1 directory should have the following structure: step1 \u2514\u2500\u2500 VEP_iPSC.mt \u251c\u2500\u2500 cols \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 entries \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 globals \u2502 \u251c\u2500\u2500 globals \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545.idx \u2502 \u251c\u2500\u2500 index \u2502 \u2514\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 README.txt \u251c\u2500\u2500 references \u251c\u2500\u2500 rows \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u2514\u2500\u2500 _SUCCESS A deatiled description of the outputs can be found in the Hail documentation","title":"Step 1: VCF to MatrixTable"},{"location":"segpy_local/#step-2-run-segregation","text":"In step 2, we will leverage the Hail ouputs from step 1 to perform segregation analysis. To run step 2 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --vcf $VCF --ped $PED After running this command, a step2 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step2 directory should have the following structure: step2 \u251c\u2500\u2500 finalseg.csv \u2514\u2500\u2500 temp The finalseg.csv file contains the variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see HEREEEEEEEEEEEEEEEEEEE for a comprehensive description of the file contents.","title":"Step 2: Run segregation"},{"location":"segpy_local/#step-3-parse-output-file","text":"In step 3, we will parse the finalseg.csv obtained from step 2 to simplify the outputs. If you simply want to remove unnecessary characters such as \" , [, ] , etc., use the following command to run step 3: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser general If you want to eliminate duplicated variant entries resulting from VEP annotations, use the following command to run step 3: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser unique After running this command, a step3 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step3 directory should have the following structure: step3 \u2514\u2500\u2500 finalseg_cleaned_general.csv \u2514\u2500\u2500 finalseg_cleaned_unique.csv The finalseg_cleaned_general.csv and finalseg_cleaned_unique.csv files contain the parsed variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see HEREEEEEEEEEEEEEEEEEEE for a comprehensive description of the file contents. Note : You can execute steps 1 to 3 sequentially, using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1-3 \\ --vcf $VCF \\ --ped $PED \\ --parser general \u2b06 back to top","title":"Step 3: Parse output file"},{"location":"segpy_slurm/","text":"Segpy SLURM In this tutorial we illustrate how to run the Segpy pipeline on a High-Performance Computing (HPC) system using the SLURM workload manager. Contents Input data Step 0: Setup Step 1: VCF to MatrixTable Step 2: Run segregation Step 3: Parse output file The following flowchart illustrates the steps for running the segregation analysis on an HPC system. Figure 1. Segpy pipeline workflow. Input data To execute the pipeline, users must provide two separate input files: VCF Pedigree file The VCF should be formatted according to the standard VCF specifications, containing information about genetic variants, including their positions, alleles, and genotype information for each individual in the study. The Pedigree file should be in .ped format, structured such that each line describes an individual in the study and includes the following columns: familyid , individualid , parentalid , maternalid , sex , phenotype , facilitating the analysis of inheritance patterns and genetic associations within the family. The familyid column must contain identifiers for the family. The individualid column must contain identifiers for the individual that match the VCF file. The parentalid column must contain identifiers for the father (0 if unknown). The maternalid column must contain identifiers for the mother (0 if unknown). The sex column must describe the biological sex of the individual (1 = male, 2 = female, 0 = unknown). The phenotype column must describe the phenotypic data (1 = unaffected, 2 = affected, -9 = missing). We provide an example .ped file HERE . Step 0: Setup Prior to initiating the pipeline, users must provide the paths to: The directory containing segpy.pip (PIPELINE_HOME) The directory designated for the pipeline's outputs (PWD) The VCF file (VCF) The pedigree file (PED) Use the following code to define these paths: # Define necessary paths export PIPELINE_HOME=path/to/segpy.pip PWD=path/to/outfolder VCF=path/to/VCF.vcf PED=path/to/pedigree.ped To ensure that segpy.pip was defined properly, run the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh -h Which should return the following: ------------------------------------ segregation pipeline version 0.0.3 is loaded ------------------- Usage: /home/sam/seg_cont/segpy003/segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -V (--verbose) = verbose output Once the necessary paths have been defined, we can initialize the pipeline using the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 0 \\ --jobmode slurm After running this command, the working directory (PWD) should have the following structure: PWD \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 segpy.config.ini \u251c\u2500\u2500 launch_summary_log.txt \u2514\u2500\u2500 logs \u251c\u2500\u2500 jobs \u2514\u2500\u2500 spark The configs directory contains a .ini file with adjustable parameters for the pipeline. Please see HERE for more information regarding these parameters. The logs directory documents the parameters and outputs from each analytical step of the Segpy pipeline to ensure reproducibility. The launch_summary_log.txt is a cumulative documentation of all submitted jobs throughout the analysis. After completing Step 0, open the segpy.config.ini file to adjust the general parameters for the analysis. The following parameters must be adjusted: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' The version of Singularity/Apptainer loaded onto your system. ACCOUNT ACCOUNT=user Your SLURM user account. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x NOTE : If you are using Segpy SLURM you must uncomment the CONTAINER_MODULE parameter. Step 1: VCF to MatrixTable In step 1, we will convert the user-provided VCF file to the Hail MatrixTable format, which is designed to efficiently store and manipulate large-scale genomic datasets. Prior to running step 1, open the segpy.config.ini file to adjust the job submission parameters for step 1: Parameter Default Explanation WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 Number of CPUs for the step 1 job sumission. THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 Amount of memory (RAM) for the step 1 job sumission. MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g Amount of time for the step 1 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, run step 1 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1 \\ --vcf $VCF After running this command, a step1 directory will be created in the working directory (PWD), which will contain the output files from Hail. The step1 directory should have the following structure: step1 \u2514\u2500\u2500 VEP_iPSC.mt \u251c\u2500\u2500 cols \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 entries \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 globals \u2502 \u251c\u2500\u2500 globals \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545.idx \u2502 \u251c\u2500\u2500 index \u2502 \u2514\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 README.txt \u251c\u2500\u2500 references \u251c\u2500\u2500 rows \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u2514\u2500\u2500 _SUCCESS A deatiled description of the outputs can be found in the Hail documentation Step 2: Run segregation In step 2, we will leverage the Hail ouputs from step 1 to perform segregation analysis. Prior to running step 2, open the segpy.config.ini file to adjust the job submission parameters for step 2: Parameter Default Explanation WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 Number of CPUs for the step 2 job sumission. THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 Amount of memory (RAM) for the step 2 job sumission. MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g Amount of time for the step 2 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, run step 2 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --vcf $VCF --ped $PED After running this command, a step2 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step2 directory should have the following structure: step2 \u251c\u2500\u2500 finalseg.csv \u2514\u2500\u2500 temp The finalseg.csv file contains the variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see HEREEEEEEEEEEEEEEEEEEE for a comprehensive description of the file contents. Step 3: Parse output file In step 3, we will parse the finalseg.csv obtained from step 2 to simplify the outputs. Prior to running step 3, open the segpy.config.ini file to adjust the job submission parameters for step 3: Parameter Default Explanation WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 Number of CPUs for the step 3 job sumission. THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 Amount of memory (RAM) for the step 3 job sumission. MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g Amount of time for the step 3 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, we can run step 3. If you simply want to remove unnecessary characters such as \" , [, ] , etc., use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser general If you want to eliminate duplicated variant entries resulting from VEP annotations, use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser unique After running this command, a step3 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step3 directory should have the following structure: step3 \u2514\u2500\u2500 finalseg_cleaned_general.csv \u2514\u2500\u2500 finalseg_cleaned_unique.csv The finalseg_cleaned_general.csv and finalseg_cleaned_unique.csv files contain the parsed variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see HEREEEEEEEEEEEEEEEEEEE for a comprehensive description of the file contents. Note : You can execute steps 1 to 3 sequentially, using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1-3 \\ --vcf $VCF \\ --ped $PED \\ --parser general \u2b06 back to top","title":"Segpy SLURM"},{"location":"segpy_slurm/#segpy-slurm","text":"In this tutorial we illustrate how to run the Segpy pipeline on a High-Performance Computing (HPC) system using the SLURM workload manager.","title":"Segpy SLURM"},{"location":"segpy_slurm/#contents","text":"Input data Step 0: Setup Step 1: VCF to MatrixTable Step 2: Run segregation Step 3: Parse output file The following flowchart illustrates the steps for running the segregation analysis on an HPC system. Figure 1. Segpy pipeline workflow.","title":"Contents"},{"location":"segpy_slurm/#input-data","text":"To execute the pipeline, users must provide two separate input files: VCF Pedigree file The VCF should be formatted according to the standard VCF specifications, containing information about genetic variants, including their positions, alleles, and genotype information for each individual in the study. The Pedigree file should be in .ped format, structured such that each line describes an individual in the study and includes the following columns: familyid , individualid , parentalid , maternalid , sex , phenotype , facilitating the analysis of inheritance patterns and genetic associations within the family. The familyid column must contain identifiers for the family. The individualid column must contain identifiers for the individual that match the VCF file. The parentalid column must contain identifiers for the father (0 if unknown). The maternalid column must contain identifiers for the mother (0 if unknown). The sex column must describe the biological sex of the individual (1 = male, 2 = female, 0 = unknown). The phenotype column must describe the phenotypic data (1 = unaffected, 2 = affected, -9 = missing). We provide an example .ped file HERE .","title":"Input data"},{"location":"segpy_slurm/#step-0-setup","text":"Prior to initiating the pipeline, users must provide the paths to: The directory containing segpy.pip (PIPELINE_HOME) The directory designated for the pipeline's outputs (PWD) The VCF file (VCF) The pedigree file (PED) Use the following code to define these paths: # Define necessary paths export PIPELINE_HOME=path/to/segpy.pip PWD=path/to/outfolder VCF=path/to/VCF.vcf PED=path/to/pedigree.ped To ensure that segpy.pip was defined properly, run the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh -h Which should return the following: ------------------------------------ segregation pipeline version 0.0.3 is loaded ------------------- Usage: /home/sam/seg_cont/segpy003/segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -V (--verbose) = verbose output Once the necessary paths have been defined, we can initialize the pipeline using the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 0 \\ --jobmode slurm After running this command, the working directory (PWD) should have the following structure: PWD \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 segpy.config.ini \u251c\u2500\u2500 launch_summary_log.txt \u2514\u2500\u2500 logs \u251c\u2500\u2500 jobs \u2514\u2500\u2500 spark The configs directory contains a .ini file with adjustable parameters for the pipeline. Please see HERE for more information regarding these parameters. The logs directory documents the parameters and outputs from each analytical step of the Segpy pipeline to ensure reproducibility. The launch_summary_log.txt is a cumulative documentation of all submitted jobs throughout the analysis. After completing Step 0, open the segpy.config.ini file to adjust the general parameters for the analysis. The following parameters must be adjusted: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' The version of Singularity/Apptainer loaded onto your system. ACCOUNT ACCOUNT=user Your SLURM user account. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x NOTE : If you are using Segpy SLURM you must uncomment the CONTAINER_MODULE parameter.","title":"Step 0: Setup"},{"location":"segpy_slurm/#step-1-vcf-to-matrixtable","text":"In step 1, we will convert the user-provided VCF file to the Hail MatrixTable format, which is designed to efficiently store and manipulate large-scale genomic datasets. Prior to running step 1, open the segpy.config.ini file to adjust the job submission parameters for step 1: Parameter Default Explanation WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 Number of CPUs for the step 1 job sumission. THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 Amount of memory (RAM) for the step 1 job sumission. MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g Amount of time for the step 1 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, run step 1 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1 \\ --vcf $VCF After running this command, a step1 directory will be created in the working directory (PWD), which will contain the output files from Hail. The step1 directory should have the following structure: step1 \u2514\u2500\u2500 VEP_iPSC.mt \u251c\u2500\u2500 cols \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 entries \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 globals \u2502 \u251c\u2500\u2500 globals \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545.idx \u2502 \u251c\u2500\u2500 index \u2502 \u2514\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 README.txt \u251c\u2500\u2500 references \u251c\u2500\u2500 rows \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u2514\u2500\u2500 _SUCCESS A deatiled description of the outputs can be found in the Hail documentation","title":"Step 1: VCF to MatrixTable"},{"location":"segpy_slurm/#step-2-run-segregation","text":"In step 2, we will leverage the Hail ouputs from step 1 to perform segregation analysis. Prior to running step 2, open the segpy.config.ini file to adjust the job submission parameters for step 2: Parameter Default Explanation WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 Number of CPUs for the step 2 job sumission. THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 Amount of memory (RAM) for the step 2 job sumission. MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g Amount of time for the step 2 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, run step 2 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --vcf $VCF --ped $PED After running this command, a step2 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step2 directory should have the following structure: step2 \u251c\u2500\u2500 finalseg.csv \u2514\u2500\u2500 temp The finalseg.csv file contains the variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see HEREEEEEEEEEEEEEEEEEEE for a comprehensive description of the file contents.","title":"Step 2: Run segregation"},{"location":"segpy_slurm/#step-3-parse-output-file","text":"In step 3, we will parse the finalseg.csv obtained from step 2 to simplify the outputs. Prior to running step 3, open the segpy.config.ini file to adjust the job submission parameters for step 3: Parameter Default Explanation WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 Number of CPUs for the step 3 job sumission. THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 Amount of memory (RAM) for the step 3 job sumission. MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g Amount of time for the step 3 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, we can run step 3. If you simply want to remove unnecessary characters such as \" , [, ] , etc., use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser general If you want to eliminate duplicated variant entries resulting from VEP annotations, use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser unique After running this command, a step3 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step3 directory should have the following structure: step3 \u2514\u2500\u2500 finalseg_cleaned_general.csv \u2514\u2500\u2500 finalseg_cleaned_unique.csv The finalseg_cleaned_general.csv and finalseg_cleaned_unique.csv files contain the parsed variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see HEREEEEEEEEEEEEEEEEEEE for a comprehensive description of the file contents. Note : You can execute steps 1 to 3 sequentially, using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1-3 \\ --vcf $VCF \\ --ped $PED \\ --parser general \u2b06 back to top","title":"Step 3: Parse output file"},{"location":"tutorial/","text":"Tutorial This section contains tutorials for segregation analysis. segpy local segpy slurm","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"This section contains tutorials for segregation analysis.","title":"Tutorial"},{"location":"tutorial/#segpy-local","text":"","title":"segpy local"},{"location":"tutorial/#segpy-slurm","text":"","title":"segpy slurm"}]}