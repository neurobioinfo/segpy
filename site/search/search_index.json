{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to segpy's documentation! Segpy is a comprehensive pipeline designed for segregation analysis. This documentation provides a step-by-step tutorial on executing segregation analysis in a High-Performance Computing (HPC) environment utilizing the Slurm workload manager system (https://slurm.schedmd.com/), or on a Linux workstation. Segregation analysis is a crucial process for exploring genetic variants within a sample of sequence data. This pipeline facilitates the counting of affected and non-affected individuals with variants, including homozygous variants, those with no variants, and those with no calls. These counts are computed both within families and globally. Additionally, the pipeline offers a detailed breakdown not only of variants but also of alleles in each case. To execute segregation analysis successfully, it is imperative to have a pedigree file with six columns: familyid , individualid , parentalid , maternalid , sex {1:male; 2:female, 0:unknown}, and phenotype ={1: control (unaffected), 2: proband(affected), -9:missing}. The genetic data must be provided in the vcf format. For guidance on how to use segpy's pipeline, consult the tutorial. Contents Installation Tutorial: segpy local segpy slurm FAQ Reference","title":"Home"},{"location":"#welcome-to-segpys-documentation","text":"Segpy is a comprehensive pipeline designed for segregation analysis. This documentation provides a step-by-step tutorial on executing segregation analysis in a High-Performance Computing (HPC) environment utilizing the Slurm workload manager system (https://slurm.schedmd.com/), or on a Linux workstation. Segregation analysis is a crucial process for exploring genetic variants within a sample of sequence data. This pipeline facilitates the counting of affected and non-affected individuals with variants, including homozygous variants, those with no variants, and those with no calls. These counts are computed both within families and globally. Additionally, the pipeline offers a detailed breakdown not only of variants but also of alleles in each case. To execute segregation analysis successfully, it is imperative to have a pedigree file with six columns: familyid , individualid , parentalid , maternalid , sex {1:male; 2:female, 0:unknown}, and phenotype ={1: control (unaffected), 2: proband(affected), -9:missing}. The genetic data must be provided in the vcf format. For guidance on how to use segpy's pipeline, consult the tutorial.","title":"Welcome to segpy's documentation!"},{"location":"#contents","text":"Installation Tutorial: segpy local segpy slurm FAQ Reference","title":"Contents"},{"location":"Acknowledgement/","text":"Acknowledgement The pipeline is done as part of MNI projects, it is written by Saeid Amiri with associate with Dan Spiegelman, and Sali Farhan at Neuro Bioinformatics Core. Copyright belongs MNI BIOINFO CORE .","title":"- Acknowledgement"},{"location":"Acknowledgement/#acknowledgement","text":"The pipeline is done as part of MNI projects, it is written by Saeid Amiri with associate with Dan Spiegelman, and Sali Farhan at Neuro Bioinformatics Core. Copyright belongs MNI BIOINFO CORE .","title":"Acknowledgement"},{"location":"FAQ/","text":"Frequently asked questions finalseg.csv finalseg.csv The output of finalseg.csv is the unpruned result from step 2. finalseg_cleaned_general.csv The output of finalseg_cleaned_general.csv is the pruned result store under step 3. The file can be categorized to 1) locus and alleles, 2) CSQ, 3) Family, 4) Family-Affected, 5) Family, 6) Family-Affected. If you do not want to have CSQ in the output file, choose CSQ=False . locus and alleles locus: chromosome alleles: a variant form of a gene CSQ VEP put all the requested information in infront CSQ. Family - Affected fam_aff_wild: Family - Affecteds: wildtype fam_aff_ncl: Family - Affecteds: no call fam_aff_vrt: Family - Affecteds: with variant fam_aff_homv: Family - Affecteds: homozygous for ALT allele Family - Nonaffected fam_naf_wild: Family - Affecteds: wildtype fam_naf_ncl: Family - Affecteds: no call fam_naf_vrt: Family - Affecteds: with variant fam_naf_homv: Family - Affecteds: homozygous for ALT allele NonFamily - Affected nfm_aff_wild: Family - Affecteds: wildtype nfm_aff_ncl: Family - Affecteds: no call nfm_aff_vrt: Family - Affecteds: with variant nfm_aff_homv: Family - Affecteds: homozygous for ALT allele NonFamily - Nonaffected nfm_naf_wild: Family - Affecteds: wildtype nfm_naf_ncl: Family - Affecteds: no call nfm_naf_vrt: Family - Affecteds: with variant nfm_naf_homv: Family - Affecteds: homozygous for ALT allele","title":"FAQ"},{"location":"FAQ/#frequently-asked-questions","text":"finalseg.csv","title":"Frequently asked questions"},{"location":"FAQ/#finalsegcsv","text":"The output of finalseg.csv is the unpruned result from step 2.","title":"finalseg.csv"},{"location":"FAQ/#finalseg_cleaned_generalcsv","text":"The output of finalseg_cleaned_general.csv is the pruned result store under step 3. The file can be categorized to 1) locus and alleles, 2) CSQ, 3) Family, 4) Family-Affected, 5) Family, 6) Family-Affected. If you do not want to have CSQ in the output file, choose CSQ=False .","title":"finalseg_cleaned_general.csv"},{"location":"FAQ/#locus-and-alleles","text":"locus: chromosome alleles: a variant form of a gene","title":"locus and alleles"},{"location":"FAQ/#csq","text":"VEP put all the requested information in infront CSQ.","title":"CSQ"},{"location":"FAQ/#family-affected","text":"fam_aff_wild: Family - Affecteds: wildtype fam_aff_ncl: Family - Affecteds: no call fam_aff_vrt: Family - Affecteds: with variant fam_aff_homv: Family - Affecteds: homozygous for ALT allele","title":"Family - Affected"},{"location":"FAQ/#family-nonaffected","text":"fam_naf_wild: Family - Affecteds: wildtype fam_naf_ncl: Family - Affecteds: no call fam_naf_vrt: Family - Affecteds: with variant fam_naf_homv: Family - Affecteds: homozygous for ALT allele","title":"Family - Nonaffected"},{"location":"FAQ/#nonfamily-affected","text":"nfm_aff_wild: Family - Affecteds: wildtype nfm_aff_ncl: Family - Affecteds: no call nfm_aff_vrt: Family - Affecteds: with variant nfm_aff_homv: Family - Affecteds: homozygous for ALT allele","title":"NonFamily - Affected"},{"location":"FAQ/#nonfamily-nonaffected","text":"nfm_naf_wild: Family - Affecteds: wildtype nfm_naf_ncl: Family - Affecteds: no call nfm_naf_vrt: Family - Affecteds: with variant nfm_naf_homv: Family - Affecteds: homozygous for ALT allele","title":"NonFamily - Nonaffected"},{"location":"LICENSE/","text":"MIT License Copyright (c) 2022 The Neuro Bioinformatics Core Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"- License"},{"location":"about/","text":"Coming soon","title":"Coming soon"},{"location":"about/#coming-soon","text":"","title":"Coming soon"},{"location":"changelog/","text":"Changelog v0.0.1 This is the initial release. v0.0.2 Supposed to Complete the documentation","title":"- Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v001","text":"This is the initial release.","title":"v0.0.1"},{"location":"changelog/#v002","text":"Supposed to Complete the documentation","title":"v0.0.2"},{"location":"contributing/","text":"Contributing This is an initial version, and any contributions or suggestions are welcomed. The pipeline is designed to be user-friendly for segregation analysis. For direct contact, please reach out to The Neuro Bioinformatics Core via [neurobioinfo@mcgill.ca]. If you encounter any issue , please report them on the GitHub repository.","title":"- Contributing"},{"location":"contributing/#contributing","text":"This is an initial version, and any contributions or suggestions are welcomed. The pipeline is designed to be user-friendly for segregation analysis. For direct contact, please reach out to The Neuro Bioinformatics Core via [neurobioinfo@mcgill.ca]. If you encounter any issue , please report them on the GitHub repository.","title":"Contributing"},{"location":"installation/","text":"Installation The pipeline is containerized and can be run using Singularity, eliminating the need to install modules within the pipeline. It is compatible with both High-Performance Computing (HPC) systems and standard Linux workstations. Contents Step 1: Apptainer Step 2: Download Step 3: Run Apptainer segpy.pip is packaged and tested with Apptainer (formerly known as Singularity) version 1.2.4. Ensure that Apptainer/Singularity is installed on your system before proceeding. Download Download the pipeline from Zenodo, which includes the segregated image. ???????? Run To test the pipeline, execute the following code bash ~/segpy.pip/launch_segpy.sh -h If you see the following the pipeline is functioning correctly. ------------------------------------ segregation pipeline version 0.0.3 is loaded ------------------- Usage: /home/sam/seg_cont/segpy003/segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -V (--verbose) = verbose output \u2b06 back to top","title":"Installation"},{"location":"installation/#installation","text":"The pipeline is containerized and can be run using Singularity, eliminating the need to install modules within the pipeline. It is compatible with both High-Performance Computing (HPC) systems and standard Linux workstations.","title":"Installation"},{"location":"installation/#contents","text":"Step 1: Apptainer Step 2: Download Step 3: Run","title":"Contents"},{"location":"installation/#apptainer","text":"segpy.pip is packaged and tested with Apptainer (formerly known as Singularity) version 1.2.4. Ensure that Apptainer/Singularity is installed on your system before proceeding.","title":"Apptainer"},{"location":"installation/#download","text":"Download the pipeline from Zenodo, which includes the segregated image. ????????","title":"Download"},{"location":"installation/#run","text":"To test the pipeline, execute the following code bash ~/segpy.pip/launch_segpy.sh -h If you see the following the pipeline is functioning correctly. ------------------------------------ segregation pipeline version 0.0.3 is loaded ------------------- Usage: /home/sam/seg_cont/segpy003/segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -V (--verbose) = verbose output \u2b06 back to top","title":"Run"},{"location":"reference/","text":"Adjustable execution parameters for the segregation pipeline Introduction Segpy parameters Introduction The configs/ directory contains the segpy.config.ini file which allows users to specify the parameters of interest explained below Segpy parameters ACCOUNT Specify user name for slurm module. SPARK_PATH Environment variables to point to the correct Spark executable ENV_PATH Environment variables to point to the correct Python executable PYTHON_CMD=python3.10 MODULEUSE The path to the environmental module JAVA_CMD=java JAVA_VERSION=11.0.2 Specify the version of Java NCOL=7 To control usage of memory to process CSQ. CSQ=True To process CSQ in the output, Consequence annotations from Ensembl VEP. GRCH=GRCh38 Genome Reference Consortium Human SPARKMEM=\"16g\" Spark Memory JAVATOOLOPTIONS=\"-Xmx8g\" Java Memory and any other options can be added to here. [step 1] Create table matrix WALLTIME_ARRAY[\"step_1\"]=00-01:00 Set a limit on the total run time of the job allocation in the step 1 under slurm. THREADS_ARRAY[\"step_1\"]=4 Request the maximum ntasks be invoked on each core in the step 1 under slurm. MEM_ARRAY[\"step_1\"]=25g Specify the real memory required per node in the step 1 under slurm. [step 2] Run segregation WALLTIME_ARRAY[\"step_2\"]=00-02:00 Set a limit on the total run time of the job allocation in the step 2 under slurm. THREADS_ARRAY[\"step_2\"]=4 Request the maximum ntasks be invoked on each core in the step 2 under slurm. MEM_ARRAY[\"step_2\"]=17g Specify the real memory required per node in the step 2 under slurm. INFO_REQUIRED=[1,2,3,4] One can determine the desired content to be included in the output 1: Global affected, 2: Global unaffected, 3: family-wise, 4: phenotype-family-wise, 5: phenotype-family-wise and non-include, 6: phenotype-family-wise-multipe sample, 7: phenotype-family-wise-multipe sample and non-include [step 3] Parsing WALLTIME_ARRAY[\"step_3\"]=00-01:00 Set a limit on the total run time of the job allocation in the step 3 under slurm. THREADS_ARRAY[\"step_3\"]=4 Request the maximum ntasks be invoked on each core in the step 3 under slurm. MEM_ARRAY[\"step_3\"]=4g Specify the real memory required per node in the step 3 under slurm.","title":"Reference"},{"location":"reference/#adjustable-execution-parameters-for-the-segregation-pipeline","text":"Introduction Segpy parameters","title":"Adjustable execution parameters for the segregation pipeline"},{"location":"reference/#introduction","text":"The configs/ directory contains the segpy.config.ini file which allows users to specify the parameters of interest explained below","title":"Introduction"},{"location":"reference/#segpy-parameters","text":"ACCOUNT Specify user name for slurm module. SPARK_PATH Environment variables to point to the correct Spark executable ENV_PATH Environment variables to point to the correct Python executable PYTHON_CMD=python3.10 MODULEUSE The path to the environmental module JAVA_CMD=java JAVA_VERSION=11.0.2 Specify the version of Java NCOL=7 To control usage of memory to process CSQ. CSQ=True To process CSQ in the output, Consequence annotations from Ensembl VEP. GRCH=GRCh38 Genome Reference Consortium Human SPARKMEM=\"16g\" Spark Memory JAVATOOLOPTIONS=\"-Xmx8g\" Java Memory and any other options can be added to here.","title":"Segpy parameters"},{"location":"reference/#step-1-create-table-matrix","text":"WALLTIME_ARRAY[\"step_1\"]=00-01:00 Set a limit on the total run time of the job allocation in the step 1 under slurm. THREADS_ARRAY[\"step_1\"]=4 Request the maximum ntasks be invoked on each core in the step 1 under slurm. MEM_ARRAY[\"step_1\"]=25g Specify the real memory required per node in the step 1 under slurm.","title":"[step 1] Create table matrix"},{"location":"reference/#step-2-run-segregation","text":"WALLTIME_ARRAY[\"step_2\"]=00-02:00 Set a limit on the total run time of the job allocation in the step 2 under slurm. THREADS_ARRAY[\"step_2\"]=4 Request the maximum ntasks be invoked on each core in the step 2 under slurm. MEM_ARRAY[\"step_2\"]=17g Specify the real memory required per node in the step 2 under slurm. INFO_REQUIRED=[1,2,3,4] One can determine the desired content to be included in the output 1: Global affected, 2: Global unaffected, 3: family-wise, 4: phenotype-family-wise, 5: phenotype-family-wise and non-include, 6: phenotype-family-wise-multipe sample, 7: phenotype-family-wise-multipe sample and non-include","title":"[step 2] Run segregation"},{"location":"reference/#step-3-parsing","text":"WALLTIME_ARRAY[\"step_3\"]=00-01:00 Set a limit on the total run time of the job allocation in the step 3 under slurm. THREADS_ARRAY[\"step_3\"]=4 Request the maximum ntasks be invoked on each core in the step 3 under slurm. MEM_ARRAY[\"step_3\"]=4g Specify the real memory required per node in the step 3 under slurm.","title":"[step 3] Parsing"},{"location":"segpy_local/","text":"Segpy on workstation segpy.pip can be used to executing on Linux workstation. Below, we illustrate the code to submit jobs for (steps 0 to 3) of the pipeline on Linux workstation. Contents Step 0: Setup Step 1: Create table matrix Step 2: Run segregation Step 3: Clean final data Note The following flowchart illustrates the steps for running the segregation analysis on Linux workstation. segpy.pip local workflow To execute the pipeline, you require 1) the path of the pipeline (PIPELINE_HOME), 2) the working directory, 3) the VCF file, and 4) the PED file. export PIPELINE_HOME=~/segpy.pip PWD=~/outfolder VCF=~/data/VEP_iPSC.vcf PED=~/data/iPSC.ped Step 0: Setup Initially, execute the following code to set up the pipeline. You can modify the parameters in ${PWD}/job_output/segpy.config.ini. sh $PIPELINE_HOME/launch_segpy.sh \\ -d ${PWD} \\ --steps 0 The parameters are: Parameter default Explanation CSQ CSQ=TRUE include CSQ information in the output by setting CSQ=TRUE. To exclude CSQ from the output, set CSQ=FALSE GRCH GRCH=GRCh38 set the reference genome AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE include all families with at least one affected sample if affecteds_only=TRUE FILTER_VARIANT FILTER_VARIANT=TRUE filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. SPARKMEM SPARKMEM=\"16g\" Amount of memory to use for the driver process, i.e. where SparkContext is initialized. Look at spark.driver.memory of \"spark.apache\" JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" specify JVM arguments as an environment variable. Look at \"JAVA_TOOL_OPTIONS\" Environment Variable Step 1: Create table matrix The following code, create MatrixTable from the VCF file. sh $PIPELINE_HOME/launch_segpy.sh \\ -d ${PWD} \\ --steps 1 \\ --vcf ${VCF} Step 2: Run segregation Execute the following code to generate the segregation. sh $PIPELINE_HOME/launch_segpy.sh \\ -d ${PWD} \\ --steps 2 \\ --vcf ${VCF} \\ --ped ${PED} Step 3: Parsing To parse the file and remove unnecessary characters such as \", [, ], etc., run the following code. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --parser general The following code eliminates duplicate information in CSQ. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --parser unique Note You can execute steps 1 to 3 sequentially, as illustrated below. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1-3 \\ --vcf ${VCF} \\ --ped ${PED} \\ --parser general \u2b06 back to top","title":"- - segpy local"},{"location":"segpy_local/#segpy-on-workstation","text":"segpy.pip can be used to executing on Linux workstation. Below, we illustrate the code to submit jobs for (steps 0 to 3) of the pipeline on Linux workstation.","title":"Segpy on workstation"},{"location":"segpy_local/#contents","text":"Step 0: Setup Step 1: Create table matrix Step 2: Run segregation Step 3: Clean final data Note The following flowchart illustrates the steps for running the segregation analysis on Linux workstation. segpy.pip local workflow To execute the pipeline, you require 1) the path of the pipeline (PIPELINE_HOME), 2) the working directory, 3) the VCF file, and 4) the PED file. export PIPELINE_HOME=~/segpy.pip PWD=~/outfolder VCF=~/data/VEP_iPSC.vcf PED=~/data/iPSC.ped","title":"Contents"},{"location":"segpy_local/#step-0-setup","text":"Initially, execute the following code to set up the pipeline. You can modify the parameters in ${PWD}/job_output/segpy.config.ini. sh $PIPELINE_HOME/launch_segpy.sh \\ -d ${PWD} \\ --steps 0 The parameters are: Parameter default Explanation CSQ CSQ=TRUE include CSQ information in the output by setting CSQ=TRUE. To exclude CSQ from the output, set CSQ=FALSE GRCH GRCH=GRCh38 set the reference genome AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE include all families with at least one affected sample if affecteds_only=TRUE FILTER_VARIANT FILTER_VARIANT=TRUE filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. SPARKMEM SPARKMEM=\"16g\" Amount of memory to use for the driver process, i.e. where SparkContext is initialized. Look at spark.driver.memory of \"spark.apache\" JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" specify JVM arguments as an environment variable. Look at \"JAVA_TOOL_OPTIONS\" Environment Variable","title":"Step 0: Setup"},{"location":"segpy_local/#step-1-create-table-matrix","text":"The following code, create MatrixTable from the VCF file. sh $PIPELINE_HOME/launch_segpy.sh \\ -d ${PWD} \\ --steps 1 \\ --vcf ${VCF}","title":"Step 1: Create table matrix"},{"location":"segpy_local/#step-2-run-segregation","text":"Execute the following code to generate the segregation. sh $PIPELINE_HOME/launch_segpy.sh \\ -d ${PWD} \\ --steps 2 \\ --vcf ${VCF} \\ --ped ${PED}","title":"Step 2: Run segregation"},{"location":"segpy_local/#step-3-parsing","text":"To parse the file and remove unnecessary characters such as \", [, ], etc., run the following code. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --parser general The following code eliminates duplicate information in CSQ. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --parser unique","title":"Step 3: Parsing"},{"location":"segpy_local/#note","text":"You can execute steps 1 to 3 sequentially, as illustrated below. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1-3 \\ --vcf ${VCF} \\ --ped ${PED} \\ --parser general \u2b06 back to top","title":"Note"},{"location":"segpy_slurm/","text":"Segpy via slurm Here, we illustrate the code to submit jobs for (steps 0 to 3) of the pipeline on an HPC system utilizing the Slurm scheduler. This pipeline has been employed on Beluga , an HPC system utilizing the Slurm system. Contents Step 0: Setup Step 1: Create table matrix Step 2: Run segregation Step 3: Clean final data The following flowchart illustrates the steps for running the segregation analysis on HPC system utilizing the Slurm system. segpy.pip slurm workflow To execute the pipeline, you require 1) the path of the pipeline (PIPELINE_HOME), 2) the working directory, 3) the VCF file, and 4) the PED file. export PIPELINE_HOME=~/segpy.slurm PWD=~/outfolder VCF=~/data/VEP_iPSC.vcf PED=~/data/iPSC.ped Step 0: Setup Initially, execute the following code to set up the pipeline. You can modify the parameters in ${PWD}/job_output/segpy.config.ini. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 0 \\ --jobmode slurm The parameters are: Parameter default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' Load the singularity/apptainer. ACCOUNT ACCOUNT=user slurm user account CSQ CSQ=TRUE include CSQ information in the output by setting CSQ=TRUE. To exclude CSQ from the output, set CSQ=FALSE GRCH GRCH=GRCh38 set the reference genome AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE include all families with at least one affected sample if affecteds_only=TRUE FILTER_VARIANT FILTER_VARIANT=TRUE filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. SPARKMEM SPARKMEM=\"16g\" Amount of memory to use for the driver process, i.e. where SparkContext is initialized. Look at spark.driver.memory of \"spark.apache\" JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" specify JVM arguments as an environment variable. Look at \"JAVA_TOOL_OPTIONS\" Environment Variable WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 number of CPUs for the job for step 1 THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 amount of memory (RAM) for the job for step 1 MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g amount of time for the job for step 1 WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 number of CPUs for the job for step 2 THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 amount of memory (RAM) for the job for step 2 MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g amount of time for the job for step 2 WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 number of CPUs for the job for step 3 THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 amount of memory (RAM) for the job for step 3 MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g amount of time for the job for step 3 Step 1: Create table matrix The following code, create MatrixTable from the VCF file. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1 \\ --vcf ${VCF} Step 2: Run segregation Execute the following code to generate the segregation. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 2 \\ --vcf ${VCF} \\ --ped ${PED} Step 3: Parsing To parse the file and remove unnecessary characters such as \", [, ], etc., run the following code. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --parser general The following code eliminates duplicate information in CSQ. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --parser unique Note You can execute steps 1 to 3 sequentially, as illustrated below. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1-3 \\ --vcf ${VCF} \\ --ped ${PED} \\ --parser general \u2b06 back to top","title":"- - segpy slurm"},{"location":"segpy_slurm/#segpy-via-slurm","text":"Here, we illustrate the code to submit jobs for (steps 0 to 3) of the pipeline on an HPC system utilizing the Slurm scheduler. This pipeline has been employed on Beluga , an HPC system utilizing the Slurm system.","title":"Segpy via slurm"},{"location":"segpy_slurm/#contents","text":"Step 0: Setup Step 1: Create table matrix Step 2: Run segregation Step 3: Clean final data The following flowchart illustrates the steps for running the segregation analysis on HPC system utilizing the Slurm system. segpy.pip slurm workflow To execute the pipeline, you require 1) the path of the pipeline (PIPELINE_HOME), 2) the working directory, 3) the VCF file, and 4) the PED file. export PIPELINE_HOME=~/segpy.slurm PWD=~/outfolder VCF=~/data/VEP_iPSC.vcf PED=~/data/iPSC.ped","title":"Contents"},{"location":"segpy_slurm/#step-0-setup","text":"Initially, execute the following code to set up the pipeline. You can modify the parameters in ${PWD}/job_output/segpy.config.ini. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 0 \\ --jobmode slurm The parameters are: Parameter default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' Load the singularity/apptainer. ACCOUNT ACCOUNT=user slurm user account CSQ CSQ=TRUE include CSQ information in the output by setting CSQ=TRUE. To exclude CSQ from the output, set CSQ=FALSE GRCH GRCH=GRCh38 set the reference genome AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE include all families with at least one affected sample if affecteds_only=TRUE FILTER_VARIANT FILTER_VARIANT=TRUE filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. SPARKMEM SPARKMEM=\"16g\" Amount of memory to use for the driver process, i.e. where SparkContext is initialized. Look at spark.driver.memory of \"spark.apache\" JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" specify JVM arguments as an environment variable. Look at \"JAVA_TOOL_OPTIONS\" Environment Variable WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 number of CPUs for the job for step 1 THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 amount of memory (RAM) for the job for step 1 MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g amount of time for the job for step 1 WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 number of CPUs for the job for step 2 THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 amount of memory (RAM) for the job for step 2 MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g amount of time for the job for step 2 WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 number of CPUs for the job for step 3 THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 amount of memory (RAM) for the job for step 3 MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g amount of time for the job for step 3","title":"Step 0: Setup"},{"location":"segpy_slurm/#step-1-create-table-matrix","text":"The following code, create MatrixTable from the VCF file. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1 \\ --vcf ${VCF}","title":"Step 1: Create table matrix"},{"location":"segpy_slurm/#step-2-run-segregation","text":"Execute the following code to generate the segregation. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 2 \\ --vcf ${VCF} \\ --ped ${PED}","title":"Step 2: Run segregation"},{"location":"segpy_slurm/#step-3-parsing","text":"To parse the file and remove unnecessary characters such as \", [, ], etc., run the following code. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --parser general The following code eliminates duplicate information in CSQ. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --parser unique","title":"Step 3: Parsing"},{"location":"segpy_slurm/#note","text":"You can execute steps 1 to 3 sequentially, as illustrated below. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1-3 \\ --vcf ${VCF} \\ --ped ${PED} \\ --parser general \u2b06 back to top","title":"Note"},{"location":"tutorial/","text":"Tutorial This section contains tutorials for segregation analysis. segpy local segpy slurm","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"This section contains tutorials for segregation analysis.","title":"Tutorial"},{"location":"tutorial/#segpy-local","text":"","title":"segpy local"},{"location":"tutorial/#segpy-slurm","text":"","title":"segpy slurm"}]}