{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Segpy's documentation! Introduction Segpy is a streamlined, user-friendly pipeline designed for variant segregation analysis, allowing investigators to compute allelic counts at variant sites across study subjects. The pipeline can be applied to both pedigree-based family cohorts \u2014 those involving single or multi-family trios, quartets, or extended families \u2014 and population-based case-control cohorts. Considering the scale of modern datasets and the computational power required for their analysis, the Segpy pipeline was designed for seamless integration with the user's high-performance computing (HPC) clusters using the SLURM Workload Manager (Segpy SLURM); however, users may also run the pipleine directly from their Linux workstation or any PC with Bash and Singularity installed (Segpy Local). As input, users must provide a single VCF file describing the genetic variants of all study subjects and a pedigree file describing the familial relationships among those individuals (if applicable) and their disease status. As output, Segpy computes variant carrier counts for affected and unaffected individuals, both within and outside of families, by categorizing wild-type individuals, heterozygous carriers, and homozygous carriers at specific loci. These counts are organized into a comprehensive data frame, with each row representing a single variant and labeled with the Sample IDs of the corresponding carriers to facilitate donwstream analysis. To meet the requirements of various study designs, Segpy integrates the ability to analyze pedigree-based and population-based datasets by providing three distinct, yet highly comparable analysis tracks: 1) Single-family 2) Multi-family 3) Case-control We provide comprehensive tutorials for running these analysis tracks in subsequent sections of this documentation. Each analysis tracks consists of four dstinct steps shown in Figure 1 . In brief, Step 0 establishes a working directory for the analyis and deposits a modifiable text file to adjust the analytical parameters. In Step 1, the user-provided VCF file is converted to the Hail MatrixTable format. In Step 2, variant segregation is performed based on the sample information defined in the user-provided pedigree file using the MatrixTable. In Step 3, the carrier counts data frame produced in Step 2 is parsed based on user specifications to reduce the computational burden of downstream analyses. Figure 1. Segpy pipeline workflow. A containerized version of the Segpy pipeline is publicly available from Zenodo , which includes the code, libraries, and dependicies required for running the analyses. In this documentation, we provide a step-by-step tutorial and explanation of the Segpy pipeline. We also provide a quick-start tutorial using a simulated dataset. Contents Tutorial: Installation Segpy SLURM Segpy Local Configuration parameters Output files FAQ About: License Changelog Contributing Acknowledgement","title":"Home"},{"location":"#welcome-to-segpys-documentation","text":"","title":"Welcome to Segpy's documentation!"},{"location":"#introduction","text":"Segpy is a streamlined, user-friendly pipeline designed for variant segregation analysis, allowing investigators to compute allelic counts at variant sites across study subjects. The pipeline can be applied to both pedigree-based family cohorts \u2014 those involving single or multi-family trios, quartets, or extended families \u2014 and population-based case-control cohorts. Considering the scale of modern datasets and the computational power required for their analysis, the Segpy pipeline was designed for seamless integration with the user's high-performance computing (HPC) clusters using the SLURM Workload Manager (Segpy SLURM); however, users may also run the pipleine directly from their Linux workstation or any PC with Bash and Singularity installed (Segpy Local). As input, users must provide a single VCF file describing the genetic variants of all study subjects and a pedigree file describing the familial relationships among those individuals (if applicable) and their disease status. As output, Segpy computes variant carrier counts for affected and unaffected individuals, both within and outside of families, by categorizing wild-type individuals, heterozygous carriers, and homozygous carriers at specific loci. These counts are organized into a comprehensive data frame, with each row representing a single variant and labeled with the Sample IDs of the corresponding carriers to facilitate donwstream analysis. To meet the requirements of various study designs, Segpy integrates the ability to analyze pedigree-based and population-based datasets by providing three distinct, yet highly comparable analysis tracks: 1) Single-family 2) Multi-family 3) Case-control We provide comprehensive tutorials for running these analysis tracks in subsequent sections of this documentation. Each analysis tracks consists of four dstinct steps shown in Figure 1 . In brief, Step 0 establishes a working directory for the analyis and deposits a modifiable text file to adjust the analytical parameters. In Step 1, the user-provided VCF file is converted to the Hail MatrixTable format. In Step 2, variant segregation is performed based on the sample information defined in the user-provided pedigree file using the MatrixTable. In Step 3, the carrier counts data frame produced in Step 2 is parsed based on user specifications to reduce the computational burden of downstream analyses. Figure 1. Segpy pipeline workflow. A containerized version of the Segpy pipeline is publicly available from Zenodo , which includes the code, libraries, and dependicies required for running the analyses. In this documentation, we provide a step-by-step tutorial and explanation of the Segpy pipeline. We also provide a quick-start tutorial using a simulated dataset.","title":"Introduction"},{"location":"#contents","text":"Tutorial: Installation Segpy SLURM Segpy Local Configuration parameters Output files FAQ About: License Changelog Contributing Acknowledgement","title":"Contents"},{"location":"Acknowledgement/","text":"Acknowledgement The pipeline is done as part of MNI projects, it is written by Saeid Amiri in association with Dan Spiegelman, Michael Fiorini , Allison Dilliott, and Sali Farhan at Neuro Bioinformatics Core. Copyright belongs MNI BIOINFO CORE .","title":"Acknowledgement"},{"location":"Acknowledgement/#acknowledgement","text":"The pipeline is done as part of MNI projects, it is written by Saeid Amiri in association with Dan Spiegelman, Michael Fiorini , Allison Dilliott, and Sali Farhan at Neuro Bioinformatics Core. Copyright belongs MNI BIOINFO CORE .","title":"Acknowledgement"},{"location":"FAQ/","text":"Frequently asked questions What is finalseg.csv? Test sample What is finalseg.csv? The file finalseg.csv is the unpruned output generated from Step 2 of the pipeline. Test sample We provide a fake test sample that users can use to practice running the pipeline. The folder is accessible here: test directory . Example files: vcf file : Example of a VCF file. Case control pedigree : Example of a pedigree file for a case-control family Multiple family pedigree : Example of a pedigree file for a multiple family single family : Example of a pedigree file for a single family","title":"FAQ"},{"location":"FAQ/#frequently-asked-questions","text":"What is finalseg.csv? Test sample","title":"Frequently asked questions"},{"location":"FAQ/#what-is-finalsegcsv","text":"The file finalseg.csv is the unpruned output generated from Step 2 of the pipeline.","title":"What is finalseg.csv?"},{"location":"FAQ/#test-sample","text":"We provide a fake test sample that users can use to practice running the pipeline. The folder is accessible here: test directory . Example files: vcf file : Example of a VCF file. Case control pedigree : Example of a pedigree file for a case-control family Multiple family pedigree : Example of a pedigree file for a multiple family single family : Example of a pedigree file for a single family","title":"Test sample"},{"location":"LICENSE/","text":"MIT License Copyright (c) 2022 The Neuro Bioinformatics Core Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/","text":"Coming soon","title":"Coming soon"},{"location":"about/#coming-soon","text":"","title":"Coming soon"},{"location":"changelog/","text":"Changelog v0.0.1 This is the initial release. v0.0.2 Update documentation v0.0.3 Implement multiple analysis tracks v0.0.6 Update output files","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v001","text":"This is the initial release.","title":"v0.0.1"},{"location":"changelog/#v002","text":"Update documentation","title":"v0.0.2"},{"location":"changelog/#v003","text":"Implement multiple analysis tracks","title":"v0.0.3"},{"location":"changelog/#v006","text":"Update output files","title":"v0.0.6"},{"location":"contributing/","text":"Contributing Any contributions or suggestions for the Segpy pipeline are welcomed. For direct contact, please reach out to The Neuro Bioinformatics Core at neurobioinfo@mcgill.ca . If you encounter any issue , please report them on the GitHub repository.","title":"Contributing"},{"location":"contributing/#contributing","text":"Any contributions or suggestions for the Segpy pipeline are welcomed. For direct contact, please reach out to The Neuro Bioinformatics Core at neurobioinfo@mcgill.ca . If you encounter any issue , please report them on the GitHub repository.","title":"Contributing"},{"location":"installation/","text":"Installation A containerized version of the Segpy pipeline is publicly available from Zenodo , which includes the code, libraries, and dependicies required for running the analyses. The container is compatible with High-Performance Computing (HPC) systems, Linux workstations, or any PC with Bash and Singularity installed. To use the Segpy pipeline, the folowing must be installed on your system: Apptainer segpy.pip Apptainer segpy.pip is packaged and tested with Apptainer (formerly Singularity) version 1.2.4. Before proceeding, ensure that Apptainer/Singularity is installed on your system. If you are using an HPC system, Apptainer is likely already installed, and you will simply need to load the module. If you encounter any issues while loading the Apptainer module, please contact your system administrator. Before running the Segpy pipeline, load the Apptainer module using the following command: # Load Apptainer module apptainer/1.2.4 Segpy.pip To download the latest version of Segpy run the following command: # Download the Segpy container curl \"https://zenodo.org/records/14503733/files/segpy.pip.zip?download=1\" --output segpy.pip.zip # Unzip the Segpy container unzip segpy.pip.zip To ensure that segpy.pip is downloaded properly, run the following command: bash /path/to/segpy.pip/launch_segpy.sh -h If segpy.pip is downloaded properly, the above command should return the folllowing: ------------------------------------ segregation pipeline version 0.0.6 is loaded ------------------- Usage: segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --analysis_mode = The default for the pipeline is analysing single or multiple family. If you want to run the pipeline on case-control, use case-control as the argumnet. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -c (--config) = config file [CURRENT: \"/scratch/fiorini9/segpy.pip/configs/segpy.config.ini\"] -V (--verbose) = verbose output ------------------- For a comprehensive help, visit https://neurobioinfo.github.io/segpy/latest/ for documentation. After successfully downloading segpy.pip we can proceed with the segregation analysis. \u2b06 back to top","title":"Installation"},{"location":"installation/#installation","text":"A containerized version of the Segpy pipeline is publicly available from Zenodo , which includes the code, libraries, and dependicies required for running the analyses. The container is compatible with High-Performance Computing (HPC) systems, Linux workstations, or any PC with Bash and Singularity installed. To use the Segpy pipeline, the folowing must be installed on your system: Apptainer segpy.pip","title":"Installation"},{"location":"installation/#apptainer","text":"segpy.pip is packaged and tested with Apptainer (formerly Singularity) version 1.2.4. Before proceeding, ensure that Apptainer/Singularity is installed on your system. If you are using an HPC system, Apptainer is likely already installed, and you will simply need to load the module. If you encounter any issues while loading the Apptainer module, please contact your system administrator. Before running the Segpy pipeline, load the Apptainer module using the following command: # Load Apptainer module apptainer/1.2.4","title":"Apptainer"},{"location":"installation/#segpypip","text":"To download the latest version of Segpy run the following command: # Download the Segpy container curl \"https://zenodo.org/records/14503733/files/segpy.pip.zip?download=1\" --output segpy.pip.zip # Unzip the Segpy container unzip segpy.pip.zip To ensure that segpy.pip is downloaded properly, run the following command: bash /path/to/segpy.pip/launch_segpy.sh -h If segpy.pip is downloaded properly, the above command should return the folllowing: ------------------------------------ segregation pipeline version 0.0.6 is loaded ------------------- Usage: segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --analysis_mode = The default for the pipeline is analysing single or multiple family. If you want to run the pipeline on case-control, use case-control as the argumnet. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -c (--config) = config file [CURRENT: \"/scratch/fiorini9/segpy.pip/configs/segpy.config.ini\"] -V (--verbose) = verbose output ------------------- For a comprehensive help, visit https://neurobioinfo.github.io/segpy/latest/ for documentation. After successfully downloading segpy.pip we can proceed with the segregation analysis. \u2b06 back to top","title":"Segpy.pip"},{"location":"output/","text":"Segpy outputs As output, Segpy computes variant counts for affected and unaffected individuals, both within and outside of families, by categorizing wild-type individuals, heterozygous carriers, and homozygous carriers at specific loci. These counts are organized into a comprehensive data frame, with each row representing a single variant and labeled with the Sample IDs of the corresponding carriers. Step 3 of the Segpy pipeline produces two output files, depending on the parsing parameter used: finalseg_cleaned_general.csv : produced using the --parser general tag to remove uncessary characters, such as \" , [, ] , etc. finalseg_cleaned_unique.csv : produced using the --parser unique tag to eliminate duplicated variant entries resulting from VEP annotations. Regardless of the parsing parameters used, the output file includes separate columns for the user-selected VEP annotations and distinct columns detailing the variant counts across individuals in the study, categorized by their disease status and family status (if applicable). The final output files will vary depending on the selected analysis track: Single-family Multi-family Case-control Single-family Column title Description affected_WT # of affected individuals that are homozygous for the wildtype allele affected_nocall # of affected individuals with no variant call affected_heterozygous # of affected individuals that are heterozygous for the variant affected_homozygous # of affected individuals that are homozygous for the variant unaffected_WT # of unaffected individuals that are homozygous for the wildtype allele unaffected_nocall # of unaffected individuals with no variant call unaffected_heterozygous # of unaffected individuals that are heterozygous for the variant unaffected_homozygous # of unaffected individuals that are homozygous for the variant affected_WT_ID Sample ID of affected individuals that are homozygous for the wildtype allele affected_nocall_ID Sample ID of affected individuals with no variant call affected_heterozygous_ID Sample ID of affected individuals that are heterozygous for the variant affected_homozygous_ID Sample ID of affected individuals that are homozygous for the variant unaffected_WT_ID Sample ID of unaffected individuals that are homozygous for the wildtype allele unaffected_nocall_ID Sample ID of unaffected individuals with no variant call unaffected_heterozygous_ID Sample ID of unaffected individuals that are heterozygous for the variant unaffected_homozygous_ID Sample ID of unaffected individuals that are homozygous for the variant affected_AF Allele frequency among affected individuals unaffected_AF Allele frequency among unaffected individuals Multi-family NOTE: For the multi-family analysis, each family will have a separate row for each variant that is observed in any individual within that family. Column title Description family_affected_WT # of affected individuals within the family that are homozygous for the wildtype allele family_affected_nocall # of affected individuals within the family with no variant call family_affected_heterozygous # of affected individuals within the family that are heterozygous for the variant family_affected_homozygous # of affected individuals within the family that are homozygous for the variant family_unaffected_WT # of unaffected individuals within the family that are homozygous for the wildtype allele family_unaffected_nocall # of unaffected individuals within the family with no variant call family_unaffected_heterozygous # of unaffected individuals within the family that are heterozygous for the variant family_unaffected_homozygous # of unaffected individuals within the family that are homozygous for the variant total_affected_WT # of affected individuals across all families that are homozygous for the wildtype allele total_affected_nocall # of affected individuals across all families with no variant call total_affected_heterozygous # of affected individuals across all families that are heterozygous for the variant total_affected_homozygous # of affected individuals across all families that are homozygous for the variant total_unaffected_WT # of unaffected individuals across all families that are homozygous for the wildtype allele total_unaffected_nocall # of unaffected individuals across all families with no variant call total_unaffected_heterozygous # of unaffected individuals across all families that are heterozygous for the variant total_unaffected_homozygous # of unaffected individuals across all families that are homozygous for the variant family_affected_WT_ID Sample ID of affected individuals within the family that are homozygous for the wildtype allele family_affected_nocall_ID Sample ID of affected individuals within the family with no variant call family_affected_heterozygous_ID Sample ID of affected individuals within the family that are heterozygous for the variant family_affected_homozygous_ID Sample ID of affected individuals within the family that are homozygous for the variant family_unaffected_WT_ID Sample ID of unaffected individuals within the family that are homozygous for the wildtype allele family_unaffected_nocall_ID Sample ID of unaffected individuals within the family with no variant call family_unaffected_heterozygous_ID Sample ID of unaffected individuals within the family that are heterozygous for the variant family_unaffected_homozygous_ID Sample ID of unaffected individuals within the family that are homozygous for the variant affected_AF Allele frequency among affected individuals unaffected_AF Allele frequency among unaffected individuals Case-control Column title Description case_WT # of cases that are homozygous for the wildtype allele case_nocall # of cases with no variant call case_heterozygous # of cases that are heterozygous for the variant case_homozygous # of cases that are homozygous for the variant control_WT # of controls that are homozygous for the wildtype allele control_nocall # of controls with no variant call control_heterozygous # of controls that are heterozygous for the variant control_homozygous # of controls that are homozygous for the variant case_WT_ID Sample ID of cases that are homozygous for the wildtype allele case_nocall_ID Sample ID of cases with no variant call case_heterozygous_ID Sample ID of cases that are heterozygous for the variant case_homozygous_ID Sample ID of cases that are homozygous for the variant control_WT_ID Sample ID of controls that are homozygous for the wildtype allele control_nocall_ID Sample ID of controls with no variant call control_heterozygous_ID Sample ID of controls that are heterozygous for the variant control_homozygous_ID Sample ID of controls that are homozygous for the variant case_AF Allele frequency among cases control_AF Allele frequency among controls","title":"Output files"},{"location":"output/#segpy-outputs","text":"As output, Segpy computes variant counts for affected and unaffected individuals, both within and outside of families, by categorizing wild-type individuals, heterozygous carriers, and homozygous carriers at specific loci. These counts are organized into a comprehensive data frame, with each row representing a single variant and labeled with the Sample IDs of the corresponding carriers. Step 3 of the Segpy pipeline produces two output files, depending on the parsing parameter used: finalseg_cleaned_general.csv : produced using the --parser general tag to remove uncessary characters, such as \" , [, ] , etc. finalseg_cleaned_unique.csv : produced using the --parser unique tag to eliminate duplicated variant entries resulting from VEP annotations. Regardless of the parsing parameters used, the output file includes separate columns for the user-selected VEP annotations and distinct columns detailing the variant counts across individuals in the study, categorized by their disease status and family status (if applicable). The final output files will vary depending on the selected analysis track: Single-family Multi-family Case-control","title":"Segpy outputs"},{"location":"output/#single-family","text":"Column title Description affected_WT # of affected individuals that are homozygous for the wildtype allele affected_nocall # of affected individuals with no variant call affected_heterozygous # of affected individuals that are heterozygous for the variant affected_homozygous # of affected individuals that are homozygous for the variant unaffected_WT # of unaffected individuals that are homozygous for the wildtype allele unaffected_nocall # of unaffected individuals with no variant call unaffected_heterozygous # of unaffected individuals that are heterozygous for the variant unaffected_homozygous # of unaffected individuals that are homozygous for the variant affected_WT_ID Sample ID of affected individuals that are homozygous for the wildtype allele affected_nocall_ID Sample ID of affected individuals with no variant call affected_heterozygous_ID Sample ID of affected individuals that are heterozygous for the variant affected_homozygous_ID Sample ID of affected individuals that are homozygous for the variant unaffected_WT_ID Sample ID of unaffected individuals that are homozygous for the wildtype allele unaffected_nocall_ID Sample ID of unaffected individuals with no variant call unaffected_heterozygous_ID Sample ID of unaffected individuals that are heterozygous for the variant unaffected_homozygous_ID Sample ID of unaffected individuals that are homozygous for the variant affected_AF Allele frequency among affected individuals unaffected_AF Allele frequency among unaffected individuals","title":"Single-family"},{"location":"output/#multi-family","text":"NOTE: For the multi-family analysis, each family will have a separate row for each variant that is observed in any individual within that family. Column title Description family_affected_WT # of affected individuals within the family that are homozygous for the wildtype allele family_affected_nocall # of affected individuals within the family with no variant call family_affected_heterozygous # of affected individuals within the family that are heterozygous for the variant family_affected_homozygous # of affected individuals within the family that are homozygous for the variant family_unaffected_WT # of unaffected individuals within the family that are homozygous for the wildtype allele family_unaffected_nocall # of unaffected individuals within the family with no variant call family_unaffected_heterozygous # of unaffected individuals within the family that are heterozygous for the variant family_unaffected_homozygous # of unaffected individuals within the family that are homozygous for the variant total_affected_WT # of affected individuals across all families that are homozygous for the wildtype allele total_affected_nocall # of affected individuals across all families with no variant call total_affected_heterozygous # of affected individuals across all families that are heterozygous for the variant total_affected_homozygous # of affected individuals across all families that are homozygous for the variant total_unaffected_WT # of unaffected individuals across all families that are homozygous for the wildtype allele total_unaffected_nocall # of unaffected individuals across all families with no variant call total_unaffected_heterozygous # of unaffected individuals across all families that are heterozygous for the variant total_unaffected_homozygous # of unaffected individuals across all families that are homozygous for the variant family_affected_WT_ID Sample ID of affected individuals within the family that are homozygous for the wildtype allele family_affected_nocall_ID Sample ID of affected individuals within the family with no variant call family_affected_heterozygous_ID Sample ID of affected individuals within the family that are heterozygous for the variant family_affected_homozygous_ID Sample ID of affected individuals within the family that are homozygous for the variant family_unaffected_WT_ID Sample ID of unaffected individuals within the family that are homozygous for the wildtype allele family_unaffected_nocall_ID Sample ID of unaffected individuals within the family with no variant call family_unaffected_heterozygous_ID Sample ID of unaffected individuals within the family that are heterozygous for the variant family_unaffected_homozygous_ID Sample ID of unaffected individuals within the family that are homozygous for the variant affected_AF Allele frequency among affected individuals unaffected_AF Allele frequency among unaffected individuals","title":"Multi-family"},{"location":"output/#case-control","text":"Column title Description case_WT # of cases that are homozygous for the wildtype allele case_nocall # of cases with no variant call case_heterozygous # of cases that are heterozygous for the variant case_homozygous # of cases that are homozygous for the variant control_WT # of controls that are homozygous for the wildtype allele control_nocall # of controls with no variant call control_heterozygous # of controls that are heterozygous for the variant control_homozygous # of controls that are homozygous for the variant case_WT_ID Sample ID of cases that are homozygous for the wildtype allele case_nocall_ID Sample ID of cases with no variant call case_heterozygous_ID Sample ID of cases that are heterozygous for the variant case_homozygous_ID Sample ID of cases that are homozygous for the variant control_WT_ID Sample ID of controls that are homozygous for the wildtype allele control_nocall_ID Sample ID of controls with no variant call control_heterozygous_ID Sample ID of controls that are heterozygous for the variant control_homozygous_ID Sample ID of controls that are homozygous for the variant case_AF Allele frequency among cases control_AF Allele frequency among controls","title":"Case-control"},{"location":"reference/","text":"Segpy configuration parameters The configs directory produced after initiating the Segpy pipeline (step 0) contains the segpy.config.ini , which contains execution parameters that users must modify prior to running their analysis. The segpy.config.ini can be modified using the following command: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x The following parameters are adjustable for the Segpy pipeline: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' The version of Singularity/Apptainer loaded onto your system. ACCOUNT ACCOUNT=user Your SLURM user account. CSQ_VEP CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant variant count values. See Step 3: Parse output file for more information. RETRIEVE_SAMPLE_ID RETRIEVE_SAMPLE_ID=TRUE Whether or not to retain SampleIDs in the final output file. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 Number of CPUs for the step 1 job sumission. THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 Amount of memory (RAM) for the step 1 job sumission. MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g Amount of time for the step 1 job sumission. WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 Number of CPUs for the step 2 job sumission. THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 Amount of memory (RAM) for the step 2 job sumission. MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g Amount of time for the step 2 job sumission. WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 Number of CPUs for the step 3 job sumission. THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 Amount of memory (RAM) for the step 3 job sumission. MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g Amount of time for the step 3 job sumission.","title":"Configuration parameters"},{"location":"reference/#segpy-configuration-parameters","text":"The configs directory produced after initiating the Segpy pipeline (step 0) contains the segpy.config.ini , which contains execution parameters that users must modify prior to running their analysis. The segpy.config.ini can be modified using the following command: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x The following parameters are adjustable for the Segpy pipeline: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' The version of Singularity/Apptainer loaded onto your system. ACCOUNT ACCOUNT=user Your SLURM user account. CSQ_VEP CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant variant count values. See Step 3: Parse output file for more information. RETRIEVE_SAMPLE_ID RETRIEVE_SAMPLE_ID=TRUE Whether or not to retain SampleIDs in the final output file. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 Number of CPUs for the step 1 job sumission. THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 Amount of memory (RAM) for the step 1 job sumission. MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g Amount of time for the step 1 job sumission. WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 Number of CPUs for the step 2 job sumission. THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 Amount of memory (RAM) for the step 2 job sumission. MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g Amount of time for the step 2 job sumission. WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 Number of CPUs for the step 3 job sumission. THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 Amount of memory (RAM) for the step 3 job sumission. MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g Amount of time for the step 3 job sumission.","title":"Segpy configuration parameters"},{"location":"segpy_local/","text":"Segpy Local In this tutorial we illustrate how to run the Segpy pipeline on Linux workstations or any PC with Bash and Singularity installed. Contents Input data Step 0: Setup Step 1: VCF to MatrixTable Step 2: Run segregation Step 3: Parse output file The following flowchart illustrates the steps for running the segregation analysis on a local Linux workstation. Figure 1. Segpy pipeline workflow. Input data To execute the pipeline, users must provide two separate input files: VCF Pedigree file The VCF should be formatted according to the standard VCF specifications, containing information about genetic variants, including their positions, alleles, and genotype information for each individual in the study. The Pedigree file should be in .ped format, structured such that each line describes an individual in the study and includes the following columns: familyid , individualid , parentalid , maternalid , sex , phenotype . The familyid column must contain identifiers for the family. The individualid column must contain identifiers for the individual that match the VCF file. The parentalid column must contain identifiers for the father (0 if unknown). The maternalid column must contain identifiers for the mother (0 if unknown). The sex column must describe the biological sex of the individual (1 = male, 2 = female, 0 = unknown). The phenotype column must describe the phenotypic data (1 = unaffected, 2 = affected, -9 = missing). We provide an example .ped file HERE . NOTE: For Case-control analyses, the familyid column should be 'case' for affected individuals and 'control' for unaffected individuals. Furthermore, the parentalid and maternalid can be '0' for every listed individual in case-control analyses. Step 0: Setup Prior to initiating the pipeline, users must provide the paths to: The directory containing segpy.pip (PIPELINE_HOME) The directory designated for the pipeline's outputs (PWD) The VCF file (VCF) The pedigree file (PED) Use the following code to define these paths: # Define necessary paths export PIPELINE_HOME=path/to/segpy.pip PWD=path/to/outfolder VCF=path/to/VCF.vcf PED=path/to/pedigree.ped To ensure that segpy.pip was defined properly, run the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh -h Which should return the following: ------------------------------------ segregation pipeline version 0.0.6 is loaded ------------------- Usage: segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --analysis_mode = The default for the pipeline is analysing single or multiple family. If you want to run the pipeline on case-control, use case-control as the argumnet. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -c (--config) = config file [CURRENT: \"/scratch/fiorini9/segpy.pip/configs/segpy.config.ini\"] -V (--verbose) = verbose output ------------------- For a comprehensive help, visit https://neurobioinfo.github.io/segpy/latest/ for documentation. Once the necessary paths have been defined, we can initialize the pipeline using the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 0 \\ --analysis_mode single_family Where analysis_mode is one of single_family , multiple_family , or case-control depending on the study design. After running this command, the working directory (PWD) should have the following structure: PWD \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 segpy.config.ini \u251c\u2500\u2500 launch_summary_log.txt \u2514\u2500\u2500 logs \u251c\u2500\u2500 jobs \u2514\u2500\u2500 spark The configs directory contains a .ini file with adjustable parameters for the pipeline. Please see the Configuration parameters section of this documentation for more information regarding the adjustable parameters. The logs directory documents the parameters and outputs from each analytical step of the Segpy pipeline to ensure reproducibility. The launch_summary_log.txt is a cumulative documentation of all submitted jobs throughout the analysis. After completing Step 0, open the segpy.config.ini file to adjust the general parameters for the analysis. The following parameters must be adjusted: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE=NA The version of Singularity/Apptainer loaded onto your system. For Local workstations, you can keep this parameter commented. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Step 1: VCF to MatrixTable In step 1, we will convert the user-provided VCF file to the Hail MatrixTable format, which is designed to efficiently store and manipulate large-scale genomic datasets. To run step 1 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1 \\ --vcf $VCF After running this command, a step1 directory will be created in the working directory (PWD), which will contain the output files from Hail. The step1 directory should have the following structure: step1 \u2514\u2500\u2500 VEP_iPSC.mt \u251c\u2500\u2500 cols \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 entries \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 globals \u2502 \u251c\u2500\u2500 globals \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545.idx \u2502 \u251c\u2500\u2500 index \u2502 \u2514\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 README.txt \u251c\u2500\u2500 references \u251c\u2500\u2500 rows \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u2514\u2500\u2500 _SUCCESS A deatiled description of the outputs can be found in the Hail documentation . Step 2: Run segregation In step 2, we will leverage the Hail ouputs from step 1 to perform segregation analysis based on the information described in the user-provided pedigree file. To run step 2 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --vcf $VCF --ped $PED After running this command, a step2 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step2 directory should have the following structure: step2 \u251c\u2500\u2500 finalseg.csv \u2514\u2500\u2500 temp The finalseg.csv file contains the variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see the Output files section of this documentation for a comprehensive description of the file contents. Step 3: Parse output file In step 3, we will parse the finalseg.csv file obtained from step 2 to simplify it and reduce its size. Once the parameters have been adjusted, we can run step 3. If you simply want to remove unnecessary characters (e.g., \" and [ ] ) use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser general If you want to eliminate duplicated variant entries resulting from VEP annotations, use the following command to run step 3: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser unique After running this command, a step3 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step3 directory should have the following structure: step3 \u2514\u2500\u2500 finalseg_cleaned_general.csv \u2514\u2500\u2500 finalseg_cleaned_unique.csv The finalseg_cleaned_general.csv and finalseg_cleaned_unique.csv files contain the parsed variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see the Output files section of this documentation for a comprehensive description of the file contents. Note : You can execute steps 1 to 3 sequentially, using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1-3 \\ --vcf $VCF \\ --ped $PED \\ --parser general \u2b06 back to top","title":"Segpy Local"},{"location":"segpy_local/#segpy-local","text":"In this tutorial we illustrate how to run the Segpy pipeline on Linux workstations or any PC with Bash and Singularity installed.","title":"Segpy Local"},{"location":"segpy_local/#contents","text":"Input data Step 0: Setup Step 1: VCF to MatrixTable Step 2: Run segregation Step 3: Parse output file The following flowchart illustrates the steps for running the segregation analysis on a local Linux workstation. Figure 1. Segpy pipeline workflow.","title":"Contents"},{"location":"segpy_local/#input-data","text":"To execute the pipeline, users must provide two separate input files: VCF Pedigree file The VCF should be formatted according to the standard VCF specifications, containing information about genetic variants, including their positions, alleles, and genotype information for each individual in the study. The Pedigree file should be in .ped format, structured such that each line describes an individual in the study and includes the following columns: familyid , individualid , parentalid , maternalid , sex , phenotype . The familyid column must contain identifiers for the family. The individualid column must contain identifiers for the individual that match the VCF file. The parentalid column must contain identifiers for the father (0 if unknown). The maternalid column must contain identifiers for the mother (0 if unknown). The sex column must describe the biological sex of the individual (1 = male, 2 = female, 0 = unknown). The phenotype column must describe the phenotypic data (1 = unaffected, 2 = affected, -9 = missing). We provide an example .ped file HERE . NOTE: For Case-control analyses, the familyid column should be 'case' for affected individuals and 'control' for unaffected individuals. Furthermore, the parentalid and maternalid can be '0' for every listed individual in case-control analyses.","title":"Input data"},{"location":"segpy_local/#step-0-setup","text":"Prior to initiating the pipeline, users must provide the paths to: The directory containing segpy.pip (PIPELINE_HOME) The directory designated for the pipeline's outputs (PWD) The VCF file (VCF) The pedigree file (PED) Use the following code to define these paths: # Define necessary paths export PIPELINE_HOME=path/to/segpy.pip PWD=path/to/outfolder VCF=path/to/VCF.vcf PED=path/to/pedigree.ped To ensure that segpy.pip was defined properly, run the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh -h Which should return the following: ------------------------------------ segregation pipeline version 0.0.6 is loaded ------------------- Usage: segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --analysis_mode = The default for the pipeline is analysing single or multiple family. If you want to run the pipeline on case-control, use case-control as the argumnet. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -c (--config) = config file [CURRENT: \"/scratch/fiorini9/segpy.pip/configs/segpy.config.ini\"] -V (--verbose) = verbose output ------------------- For a comprehensive help, visit https://neurobioinfo.github.io/segpy/latest/ for documentation. Once the necessary paths have been defined, we can initialize the pipeline using the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 0 \\ --analysis_mode single_family Where analysis_mode is one of single_family , multiple_family , or case-control depending on the study design. After running this command, the working directory (PWD) should have the following structure: PWD \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 segpy.config.ini \u251c\u2500\u2500 launch_summary_log.txt \u2514\u2500\u2500 logs \u251c\u2500\u2500 jobs \u2514\u2500\u2500 spark The configs directory contains a .ini file with adjustable parameters for the pipeline. Please see the Configuration parameters section of this documentation for more information regarding the adjustable parameters. The logs directory documents the parameters and outputs from each analytical step of the Segpy pipeline to ensure reproducibility. The launch_summary_log.txt is a cumulative documentation of all submitted jobs throughout the analysis. After completing Step 0, open the segpy.config.ini file to adjust the general parameters for the analysis. The following parameters must be adjusted: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE=NA The version of Singularity/Apptainer loaded onto your system. For Local workstations, you can keep this parameter commented. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x","title":"Step 0: Setup"},{"location":"segpy_local/#step-1-vcf-to-matrixtable","text":"In step 1, we will convert the user-provided VCF file to the Hail MatrixTable format, which is designed to efficiently store and manipulate large-scale genomic datasets. To run step 1 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1 \\ --vcf $VCF After running this command, a step1 directory will be created in the working directory (PWD), which will contain the output files from Hail. The step1 directory should have the following structure: step1 \u2514\u2500\u2500 VEP_iPSC.mt \u251c\u2500\u2500 cols \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 entries \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 globals \u2502 \u251c\u2500\u2500 globals \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545.idx \u2502 \u251c\u2500\u2500 index \u2502 \u2514\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 README.txt \u251c\u2500\u2500 references \u251c\u2500\u2500 rows \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u2514\u2500\u2500 _SUCCESS A deatiled description of the outputs can be found in the Hail documentation .","title":"Step 1: VCF to MatrixTable"},{"location":"segpy_local/#step-2-run-segregation","text":"In step 2, we will leverage the Hail ouputs from step 1 to perform segregation analysis based on the information described in the user-provided pedigree file. To run step 2 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --vcf $VCF --ped $PED After running this command, a step2 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step2 directory should have the following structure: step2 \u251c\u2500\u2500 finalseg.csv \u2514\u2500\u2500 temp The finalseg.csv file contains the variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see the Output files section of this documentation for a comprehensive description of the file contents.","title":"Step 2: Run segregation"},{"location":"segpy_local/#step-3-parse-output-file","text":"In step 3, we will parse the finalseg.csv file obtained from step 2 to simplify it and reduce its size. Once the parameters have been adjusted, we can run step 3. If you simply want to remove unnecessary characters (e.g., \" and [ ] ) use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser general If you want to eliminate duplicated variant entries resulting from VEP annotations, use the following command to run step 3: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --parser unique After running this command, a step3 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step3 directory should have the following structure: step3 \u2514\u2500\u2500 finalseg_cleaned_general.csv \u2514\u2500\u2500 finalseg_cleaned_unique.csv The finalseg_cleaned_general.csv and finalseg_cleaned_unique.csv files contain the parsed variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see the Output files section of this documentation for a comprehensive description of the file contents. Note : You can execute steps 1 to 3 sequentially, using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1-3 \\ --vcf $VCF \\ --ped $PED \\ --parser general \u2b06 back to top","title":"Step 3: Parse output file"},{"location":"segpy_slurm/","text":"Segpy SLURM In this tutorial we illustrate how to run the Segpy pipeline on a High-Performance Computing (HPC) system using the SLURM workload manager. Contents Input data Step 0: Setup Step 1: VCF to MatrixTable Step 2: Run segregation Step 3: Parse output file The following flowchart illustrates the steps for running the segregation analysis on an HPC system. Figure 1. Segpy pipeline workflow. Input data To execute the pipeline, users must provide two separate input files: VCF Pedigree file The VCF should be formatted according to the standard VCF specifications, containing information about genetic variants, including their positions, alleles, and genotype information for each individual in the study. The Pedigree file should be in .ped format, structured such that each line describes an individual in the study and includes the following columns: familyid , individualid , parentalid , maternalid , sex , phenotype . The familyid column must contain identifiers for the family. The individualid column must contain identifiers for the individual that match the VCF file. The parentalid column must contain identifiers for the father (0 if unknown). The maternalid column must contain identifiers for the mother (0 if unknown). The sex column must describe the biological sex of the individual (1 = male, 2 = female, 0 = unknown). The phenotype column must describe the phenotypic data (1 = unaffected, 2 = affected, -9 = missing). We provide an example .ped file HERE . NOTE: For Case-control analyses, the familyid column should be 'case' for affected individuals and 'control' for unaffected individuals. Furthermore, the parentalid and maternalid can be '0' for every listed individual in case-control analyses. Step 0: Setup Prior to initiating the pipeline, users must provide the paths to: The directory containing segpy.pip (PIPELINE_HOME) The directory designated for the pipeline's outputs (PWD) The VCF file (VCF) The pedigree file (PED) Use the following code to define these paths: # Define necessary paths export PIPELINE_HOME=path/to/segpy.pip PWD=path/to/outfolder VCF=path/to/VCF.vcf PED=path/to/pedigree.ped To ensure that segpy.pip was defined properly, run the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh -h Which should return the following: ------------------------------------ segregation pipeline version 0.0.6 is loaded ------------------- Usage: segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --analysis_mode = The default for the pipeline is analysing single or multiple family. If you want to run the pipeline on case-control, use case-control as the argumnet. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -c (--config) = config file [CURRENT: \"/scratch/fiorini9/segpy.pip/configs/segpy.config.ini\"] -V (--verbose) = verbose output ------------------- For a comprehensive help, visit https://neurobioinfo.github.io/segpy/latest/ for documentation. Once the necessary paths have been defined, we can initialize the pipeline using the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 0 \\ --analysis_mode single_family \\ --jobmode slurm Where analysis_mode is one of single_family , multiple_family , or case-control depending on the study design. After running this command, the working directory (PWD) should have the following structure: PWD \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 segpy.config.ini \u251c\u2500\u2500 launch_summary_log.txt \u2514\u2500\u2500 logs \u251c\u2500\u2500 jobs \u2514\u2500\u2500 spark The configs directory contains a .ini file with adjustable parameters for the pipeline. Please see the Configuration parameters section of this documentation for more information regarding the adjustable parameters. The logs directory documents the parameters and outputs from each analytical step of the Segpy pipeline to ensure reproducibility. The launch_summary_log.txt is a cumulative documentation of all submitted jobs throughout the analysis. After completing Step 0, open the segpy.config.ini file to adjust the general parameters for the analysis. The following parameters must be adjusted: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' The version of Singularity/Apptainer loaded onto your system. ACCOUNT ACCOUNT=user Your SLURM user account. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x NOTE : If you are using Segpy SLURM you must uncomment the CONTAINER_MODULE parameter. Step 1: VCF to MatrixTable In step 1, we will convert the user-provided VCF file to the Hail MatrixTable format, which is designed to efficiently store and manipulate large-scale genomic datasets. Prior to running step 1, open the segpy.config.ini file to adjust the job submission parameters for step 1: Parameter Default Explanation WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 Number of CPUs for the step 1 job sumission. THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 Amount of memory (RAM) for the step 1 job sumission. MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g Amount of time for the step 1 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, run step 1 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1 \\ --vcf $VCF After running this command, a step1 directory will be created in the working directory (PWD), which will contain the output files from Hail. The step1 directory should have the following structure: step1 \u2514\u2500\u2500 VEP_iPSC.mt \u251c\u2500\u2500 cols \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 entries \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 globals \u2502 \u251c\u2500\u2500 globals \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545.idx \u2502 \u251c\u2500\u2500 index \u2502 \u2514\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 README.txt \u251c\u2500\u2500 references \u251c\u2500\u2500 rows \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u2514\u2500\u2500 _SUCCESS A deatiled description of the outputs can be found in the Hail documentation . Step 2: Run segregation In step 2, we will leverage the Hail ouputs from step 1 to perform segregation analysis based on the information described in the user-provided pedigree file. Prior to running step 2, open the segpy.config.ini file to adjust the job submission parameters for step 2: Parameter Default Explanation WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 Number of CPUs for the step 2 job sumission. THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 Amount of memory (RAM) for the step 2 job sumission. MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g Amount of time for the step 2 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, run step 2 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --vcf $VCF --ped $PED After running this command, a step2 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step2 directory should have the following structure: step2 \u251c\u2500\u2500 finalseg.csv \u2514\u2500\u2500 temp The finalseg.csv file contains the variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see the Output files section of this documentation for a comprehensive description of the file contents. Step 3: Parse output file In step 3, we will parse the finalseg.csv file obtained from step 2 to simplify it and reduce its size. Prior to running step 3, open the segpy.config.ini file to adjust the job submission parameters for step 3: Parameter Default Explanation WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 Number of CPUs for the step 3 job sumission. THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 Amount of memory (RAM) for the step 3 job sumission. MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g Amount of time for the step 3 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, we can run step 3. If you simply want to remove unnecessary characters (e.g., \" and [ ] ) use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 3 \\ --parser general If you want to eliminate duplicated variant entries resulting from VEP annotations, use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 3 \\ --parser unique After running this command, a step3 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step3 directory should have the following structure: step3 \u2514\u2500\u2500 finalseg_cleaned_general.csv \u2514\u2500\u2500 finalseg_cleaned_unique.csv The finalseg_cleaned_general.csv and finalseg_cleaned_unique.csv files contain the parsed variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see the Output files section of this documentation for a comprehensive description of the file contents. Note : You can execute steps 1 to 3 sequentially, using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1-3 \\ --vcf $VCF \\ --ped $PED \\ --parser general \u2b06 back to top","title":"Segpy SLURM"},{"location":"segpy_slurm/#segpy-slurm","text":"In this tutorial we illustrate how to run the Segpy pipeline on a High-Performance Computing (HPC) system using the SLURM workload manager.","title":"Segpy SLURM"},{"location":"segpy_slurm/#contents","text":"Input data Step 0: Setup Step 1: VCF to MatrixTable Step 2: Run segregation Step 3: Parse output file The following flowchart illustrates the steps for running the segregation analysis on an HPC system. Figure 1. Segpy pipeline workflow.","title":"Contents"},{"location":"segpy_slurm/#input-data","text":"To execute the pipeline, users must provide two separate input files: VCF Pedigree file The VCF should be formatted according to the standard VCF specifications, containing information about genetic variants, including their positions, alleles, and genotype information for each individual in the study. The Pedigree file should be in .ped format, structured such that each line describes an individual in the study and includes the following columns: familyid , individualid , parentalid , maternalid , sex , phenotype . The familyid column must contain identifiers for the family. The individualid column must contain identifiers for the individual that match the VCF file. The parentalid column must contain identifiers for the father (0 if unknown). The maternalid column must contain identifiers for the mother (0 if unknown). The sex column must describe the biological sex of the individual (1 = male, 2 = female, 0 = unknown). The phenotype column must describe the phenotypic data (1 = unaffected, 2 = affected, -9 = missing). We provide an example .ped file HERE . NOTE: For Case-control analyses, the familyid column should be 'case' for affected individuals and 'control' for unaffected individuals. Furthermore, the parentalid and maternalid can be '0' for every listed individual in case-control analyses.","title":"Input data"},{"location":"segpy_slurm/#step-0-setup","text":"Prior to initiating the pipeline, users must provide the paths to: The directory containing segpy.pip (PIPELINE_HOME) The directory designated for the pipeline's outputs (PWD) The VCF file (VCF) The pedigree file (PED) Use the following code to define these paths: # Define necessary paths export PIPELINE_HOME=path/to/segpy.pip PWD=path/to/outfolder VCF=path/to/VCF.vcf PED=path/to/pedigree.ped To ensure that segpy.pip was defined properly, run the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh -h Which should return the following: ------------------------------------ segregation pipeline version 0.0.6 is loaded ------------------- Usage: segpy.pip/launch_segpy.sh [arguments] mandatory arguments: -d (--dir) = Working directory (where all the outputs will be printed) (give full path) -s (--steps) = Specify what steps, e.g., 2 to run just step 2, 1-3 (run steps 1 through 3). 'ALL' to run all steps. steps: 0: initial setup 1: create hail matrix 2: run segregation 3: final cleanup and formatting optional arguments: -h (--help) = Get the program options and exit. --jobmode = The default for the pipeline is local. If you want to run the pipeline on slurm system, use slurm as the argument. --analysis_mode = The default for the pipeline is analysing single or multiple family. If you want to run the pipeline on case-control, use case-control as the argumnet. --parser = 'general': to general parsing, 'unique': drop multiplicities -v (--vcf) = VCF file (mandatory for steps 1-3) -p (--ped) = PED file (mandatory for steps 1-3) -c (--config) = config file [CURRENT: \"/scratch/fiorini9/segpy.pip/configs/segpy.config.ini\"] -V (--verbose) = verbose output ------------------- For a comprehensive help, visit https://neurobioinfo.github.io/segpy/latest/ for documentation. Once the necessary paths have been defined, we can initialize the pipeline using the following command: module load apptainer/1.2.4 bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 0 \\ --analysis_mode single_family \\ --jobmode slurm Where analysis_mode is one of single_family , multiple_family , or case-control depending on the study design. After running this command, the working directory (PWD) should have the following structure: PWD \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 segpy.config.ini \u251c\u2500\u2500 launch_summary_log.txt \u2514\u2500\u2500 logs \u251c\u2500\u2500 jobs \u2514\u2500\u2500 spark The configs directory contains a .ini file with adjustable parameters for the pipeline. Please see the Configuration parameters section of this documentation for more information regarding the adjustable parameters. The logs directory documents the parameters and outputs from each analytical step of the Segpy pipeline to ensure reproducibility. The launch_summary_log.txt is a cumulative documentation of all submitted jobs throughout the analysis. After completing Step 0, open the segpy.config.ini file to adjust the general parameters for the analysis. The following parameters must be adjusted: Parameter Default Explanation CONTAINER_MODULE CONTAINER_MODULE='apptainer/1.2.4' The version of Singularity/Apptainer loaded onto your system. ACCOUNT ACCOUNT=user Your SLURM user account. CSQ CSQ=TRUE Whether or not to include variant annotations from VEP in the output file. GRCH GRCH=GRCh38 Reference genome version (GRCh38 or GRCh37). AFFECTEDS_ONLY AFFECTEDS_ONLY=FALSE Whether or not to only include families with at least one affected individual. FILTER_VARIANT FILTER_VARIANT=TRUE Filter the output file using relevant counting column values where 'fam_aff_vrt'+'fam_aff_homv'+'fam_naf_vrt'+'fam_naf_homv' >0. See Step 3: Parse output file for more information. JAVATOOLOPTIONS JAVATOOLOPTIONS=\"-Xmx6g\" Java Virtual Machine (JVM) configuration setting. For most use cases the default value will be appropriate. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x NOTE : If you are using Segpy SLURM you must uncomment the CONTAINER_MODULE parameter.","title":"Step 0: Setup"},{"location":"segpy_slurm/#step-1-vcf-to-matrixtable","text":"In step 1, we will convert the user-provided VCF file to the Hail MatrixTable format, which is designed to efficiently store and manipulate large-scale genomic datasets. Prior to running step 1, open the segpy.config.ini file to adjust the job submission parameters for step 1: Parameter Default Explanation WALLTIME_ARRAY[\"step1\"] WALLTIME_ARRAY[\"step1\"]=00-5:00 Number of CPUs for the step 1 job sumission. THREADS_ARRAY[\"step1\"] THREADS_ARRAY[\"step1\"]=8 Amount of memory (RAM) for the step 1 job sumission. MEM_ARRAY[\"step1\"] MEM_ARRAY[\"step1\"]=10g Amount of time for the step 1 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, run step 1 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1 \\ --vcf $VCF After running this command, a step1 directory will be created in the working directory (PWD), which will contain the output files from Hail. The step1 directory should have the following structure: step1 \u2514\u2500\u2500 VEP_iPSC.mt \u251c\u2500\u2500 cols \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 entries \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 globals \u2502 \u251c\u2500\u2500 globals \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u2514\u2500\u2500 part-0 \u2502 \u2514\u2500\u2500 _SUCCESS \u251c\u2500\u2500 index \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f.idx \u2502 \u2502 \u251c\u2500\u2500 index \u2502 \u2502 \u2514\u2500\u2500 metadata.json.gz \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545.idx \u2502 \u251c\u2500\u2500 index \u2502 \u2514\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 metadata.json.gz \u251c\u2500\u2500 README.txt \u251c\u2500\u2500 references \u251c\u2500\u2500 rows \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u251c\u2500\u2500 README.txt \u2502 \u251c\u2500\u2500 rows \u2502 \u2502 \u251c\u2500\u2500 metadata.json.gz \u2502 \u2502 \u2514\u2500\u2500 parts \u2502 \u2502 \u251c\u2500\u2500 part-0-5eb5d5aa-c7b2-4acf-bea1-b55e6f4d9bff \u2502 \u2502 \u251c\u2500\u2500 part-1-8ba09e12-618c-416b-8abe-8dd858760e2f \u2502 \u2502 \u2514\u2500\u2500 part-2-7b8e9e61-db1b-4697-8e27-c797f9082545 \u2502 \u2514\u2500\u2500 _SUCCESS \u2514\u2500\u2500 _SUCCESS A deatiled description of the outputs can be found in the Hail documentation .","title":"Step 1: VCF to MatrixTable"},{"location":"segpy_slurm/#step-2-run-segregation","text":"In step 2, we will leverage the Hail ouputs from step 1 to perform segregation analysis based on the information described in the user-provided pedigree file. Prior to running step 2, open the segpy.config.ini file to adjust the job submission parameters for step 2: Parameter Default Explanation WALLTIME_ARRAY[\"step2\"] WALLTIME_ARRAY[\"step2\"]=00-5:00 Number of CPUs for the step 2 job sumission. THREADS_ARRAY[\"step2\"] THREADS_ARRAY[\"step2\"]=8 Amount of memory (RAM) for the step 2 job sumission. MEM_ARRAY[\"step2\"] MEM_ARRAY[\"step2\"]=10g Amount of time for the step 2 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, run step 2 using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 2 \\ --vcf $VCF --ped $PED After running this command, a step2 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step2 directory should have the following structure: step2 \u251c\u2500\u2500 finalseg.csv \u2514\u2500\u2500 temp The finalseg.csv file contains the variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see the Output files section of this documentation for a comprehensive description of the file contents.","title":"Step 2: Run segregation"},{"location":"segpy_slurm/#step-3-parse-output-file","text":"In step 3, we will parse the finalseg.csv file obtained from step 2 to simplify it and reduce its size. Prior to running step 3, open the segpy.config.ini file to adjust the job submission parameters for step 3: Parameter Default Explanation WALLTIME_ARRAY[\"step3\"] WALLTIME_ARRAY[\"step3\"]=00-5:00 Number of CPUs for the step 3 job sumission. THREADS_ARRAY[\"step3\"] THREADS_ARRAY[\"step3\"]=8 Amount of memory (RAM) for the step 3 job sumission. MEM_ARRAY[\"step3\"] MEM_ARRAY[\"step3\"]=10g Amount of time for the step 3 job sumission. Use the following code to modify the segpy.config.ini file: # 1) Open .ini file to edit with nano nano $PWD/configs/segpy.config.ini # 2) Make appropriate changes. # 3) Save and exit the .ini file with ctrl+0, enter ,ctrl+x Once the parameters have been adjusted, we can run step 3. If you simply want to remove unnecessary characters (e.g., \" and [ ] ) use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 3 \\ --parser general If you want to eliminate duplicated variant entries resulting from VEP annotations, use the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 3 \\ --parser unique After running this command, a step3 directory will be created in the working directory (PWD), which will contain the output files from the segregation analysis. The step3 directory should have the following structure: step3 \u2514\u2500\u2500 finalseg_cleaned_general.csv \u2514\u2500\u2500 finalseg_cleaned_unique.csv The finalseg_cleaned_general.csv and finalseg_cleaned_unique.csv files contain the parsed variant counts obtained from the segregation analysis, as well as the variant annotations supplied by VEP. Please see the Output files section of this documentation for a comprehensive description of the file contents. Note : You can execute steps 1 to 3 sequentially, using the following command: bash $PIPELINE_HOME/launch_segpy.sh \\ -d $PWD \\ --steps 1-3 \\ --vcf $VCF \\ --ped $PED \\ --parser general \u2b06 back to top","title":"Step 3: Parse output file"},{"location":"tutorial/","text":"Tutorial This section contains tutorials for segregation analysis. segpy local segpy slurm","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"This section contains tutorials for segregation analysis.","title":"Tutorial"},{"location":"tutorial/#segpy-local","text":"","title":"segpy local"},{"location":"tutorial/#segpy-slurm","text":"","title":"segpy slurm"}]}