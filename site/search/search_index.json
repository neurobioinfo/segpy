{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to segpy's documentation! Segpy is a comprehensive pipeline designed for segregation analysis. This documentation provides a step-by-step tutorial on executing segregation analysis in a High-Performance Computing (HPC) environment utilizing the Slurm workload manager system (https://slurm.schedmd.com/), on a Linux workstation, or directly through the segpy module in Python. Segregation analysis is a crucial process for exploring genetic variants within a sample of sequence data. This pipeline facilitates the counting of affected and non-affected individuals with variants, including homozygous variants, those with no variants, and those with no calls. These counts are computed both within families and globally. Additionally, the pipeline offers a detailed breakdown not only of variants but also of alleles in each case. To execute segregation analysis successfully, it is imperative to have a pedigree file with six columns: familyid , individualid , parentalid , maternalid , sex {1:male; 2:female, 0:unknown}, and phenotype ={1: control (unaffected), 2: proband(affected), -9:missing}. The genetic data must be provided in the vcf format. For guidance on how to use segpy's pipeline, refer to the tutorial. Contents Installation Tutorial: segpy segpy.slurm FAQ Reference","title":"Home"},{"location":"#welcome-to-segpys-documentation","text":"Segpy is a comprehensive pipeline designed for segregation analysis. This documentation provides a step-by-step tutorial on executing segregation analysis in a High-Performance Computing (HPC) environment utilizing the Slurm workload manager system (https://slurm.schedmd.com/), on a Linux workstation, or directly through the segpy module in Python. Segregation analysis is a crucial process for exploring genetic variants within a sample of sequence data. This pipeline facilitates the counting of affected and non-affected individuals with variants, including homozygous variants, those with no variants, and those with no calls. These counts are computed both within families and globally. Additionally, the pipeline offers a detailed breakdown not only of variants but also of alleles in each case. To execute segregation analysis successfully, it is imperative to have a pedigree file with six columns: familyid , individualid , parentalid , maternalid , sex {1:male; 2:female, 0:unknown}, and phenotype ={1: control (unaffected), 2: proband(affected), -9:missing}. The genetic data must be provided in the vcf format. For guidance on how to use segpy's pipeline, refer to the tutorial.","title":"Welcome to segpy's documentation!"},{"location":"#contents","text":"Installation Tutorial: segpy segpy.slurm FAQ Reference","title":"Contents"},{"location":"Acknowledgement/","text":"Acknowledgement The pipeline is done as part of MNI projects, it is written by Saeid Amiri with associate with Dan Spiegelman, and Sali Farhan at Neuro Bioinformatics Core. Copyright belongs MNI BIOINFO CORE .","title":"- Acknowledgement"},{"location":"Acknowledgement/#acknowledgement","text":"The pipeline is done as part of MNI projects, it is written by Saeid Amiri with associate with Dan Spiegelman, and Sali Farhan at Neuro Bioinformatics Core. Copyright belongs MNI BIOINFO CORE .","title":"Acknowledgement"},{"location":"FAQ/","text":"Frequently asked questions What is finalseg.csv What is finalseg.csv The output of finalseg.csv can be categorized to 1) locus and alleles, 2) CSQ, 3) Global- Non-Affected 4) Global-Affected, 5) Family, 6) Family-Affected 7) Family - Non-affected. If you do not want to have CSQ in the output file, choose CSQ=False . locus and alleles locus: chromosome alleles: a variant form of a gene CSQ VEP put all the requested information in infront CSQ. Global - Non-Affected glb_naf_wild: Global - Non-Affecteds, wildtype glb_naf_ncl: Global - Non-Affecteds, no call glb_naf_vrt: Global - Non-Affecteds, with variant glb_naf_homv: Global - Non-Affecteds, homozygous for ALT allele glb_naf_altaf: Global - Non-Affecteds, ALT allele frequency Global - Affected glb_aff_wild: Global - Affecteds, wildtype glb_aff_ncl: Global - Affecteds, no call glb_aff_vrt: Global - Affecteds, with variant glb_aff_homv: Global - Affecteds, homozygous for ALT allele glb_aff_altaf: Global - Affecteds, ALT allele frequency Family {famid}_wild: Family - Affecteds: wildtype {famid}_ncl: Family - Affecteds: no call {famid}_vrt: Family - Affecteds: with variant {famid}_homv: Family - Affecteds: homozygous for ALT allele Family - Affected {famid}_wild_aff: Family - Affecteds: wildtype {famid}_ncl_aff: Family - Affecteds: no call {famid}_vrt_aff: Family - Affecteds: with variant {famid}_homv_aff: Family - Affecteds: homozygous for ALT allele Family - Nonaffected {famid}_wild_naf: Family - Nonaffecteds: wildtype {famid}_ncl_naf: Family - Nonaffecteds: no call {famid}_vrt_naf: Family - Nonaffecteds: with variant {famid}_homv_naf: Family - Nonaffecteds: homozygous for ALT allele","title":"FAQ"},{"location":"FAQ/#frequently-asked-questions","text":"What is finalseg.csv","title":"Frequently asked questions"},{"location":"FAQ/#what-is-finalsegcsv","text":"The output of finalseg.csv can be categorized to 1) locus and alleles, 2) CSQ, 3) Global- Non-Affected 4) Global-Affected, 5) Family, 6) Family-Affected 7) Family - Non-affected. If you do not want to have CSQ in the output file, choose CSQ=False .","title":"What is finalseg.csv"},{"location":"FAQ/#locus-and-alleles","text":"locus: chromosome alleles: a variant form of a gene","title":"locus and alleles"},{"location":"FAQ/#csq","text":"VEP put all the requested information in infront CSQ.","title":"CSQ"},{"location":"FAQ/#global-non-affected","text":"glb_naf_wild: Global - Non-Affecteds, wildtype glb_naf_ncl: Global - Non-Affecteds, no call glb_naf_vrt: Global - Non-Affecteds, with variant glb_naf_homv: Global - Non-Affecteds, homozygous for ALT allele glb_naf_altaf: Global - Non-Affecteds, ALT allele frequency","title":"Global - Non-Affected"},{"location":"FAQ/#global-affected","text":"glb_aff_wild: Global - Affecteds, wildtype glb_aff_ncl: Global - Affecteds, no call glb_aff_vrt: Global - Affecteds, with variant glb_aff_homv: Global - Affecteds, homozygous for ALT allele glb_aff_altaf: Global - Affecteds, ALT allele frequency","title":"Global - Affected"},{"location":"FAQ/#family","text":"{famid}_wild: Family - Affecteds: wildtype {famid}_ncl: Family - Affecteds: no call {famid}_vrt: Family - Affecteds: with variant {famid}_homv: Family - Affecteds: homozygous for ALT allele","title":"Family"},{"location":"FAQ/#family-affected","text":"{famid}_wild_aff: Family - Affecteds: wildtype {famid}_ncl_aff: Family - Affecteds: no call {famid}_vrt_aff: Family - Affecteds: with variant {famid}_homv_aff: Family - Affecteds: homozygous for ALT allele","title":"Family - Affected"},{"location":"FAQ/#family-nonaffected","text":"{famid}_wild_naf: Family - Nonaffecteds: wildtype {famid}_ncl_naf: Family - Nonaffecteds: no call {famid}_vrt_naf: Family - Nonaffecteds: with variant {famid}_homv_naf: Family - Nonaffecteds: homozygous for ALT allele","title":"Family - Nonaffected"},{"location":"about/","text":"Coming soon","title":"Coming soon"},{"location":"about/#coming-soon","text":"","title":"Coming soon"},{"location":"changelog/","text":"Changelog v0.1.1 This is the initial release. v0.2.0 v0.2.1 Supposed to Complete the documentation","title":"- Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v011","text":"This is the initial release.","title":"v0.1.1"},{"location":"changelog/#v020","text":"","title":"v0.2.0"},{"location":"changelog/#v021","text":"Supposed to Complete the documentation","title":"v0.2.1"},{"location":"contributing/","text":"Contributing This is an early version, any contribute or suggestion is appreciated, it is supposed to be an easy-to-use pipline for scRNA analysis, so you can directly contact with Saeid Amiri . If you encounter any issue , please file its Github repository.","title":"- Contributing"},{"location":"contributing/#contributing","text":"This is an early version, any contribute or suggestion is appreciated, it is supposed to be an easy-to-use pipline for scRNA analysis, so you can directly contact with Saeid Amiri . If you encounter any issue , please file its Github repository.","title":"Contributing"},{"location":"installation/","text":"Installation segpy is a python module developed on Python 3.10.2 to run the segregation analysis, the module can be easily downloaded using pip : pip install 'git+https://github.com/neurobioinfo/segpy#subdirectory=segpy' The segregation can do done using the segpy module, if you have access to HPC, you can automate it using segpy.slurm The segpy.slurm is written in the bash, so it can be used with any slurm system. To download segpy.slurm , run the below comments wget https://github.com/neurobioinfo/segpy/releases/download/v0.2.1/segpy.slurm.zip unzip segpy.slurm.zip To obtain a brief guidance of the pipeline, execute the following code. bash ./scrnabox.slurm/launch_segpy.sh -h","title":"Installation"},{"location":"installation/#installation","text":"segpy is a python module developed on Python 3.10.2 to run the segregation analysis, the module can be easily downloaded using pip : pip install 'git+https://github.com/neurobioinfo/segpy#subdirectory=segpy' The segregation can do done using the segpy module, if you have access to HPC, you can automate it using segpy.slurm The segpy.slurm is written in the bash, so it can be used with any slurm system. To download segpy.slurm , run the below comments wget https://github.com/neurobioinfo/segpy/releases/download/v0.2.1/segpy.slurm.zip unzip segpy.slurm.zip To obtain a brief guidance of the pipeline, execute the following code. bash ./scrnabox.slurm/launch_segpy.sh -h","title":"Installation"},{"location":"reference/","text":"Reference","title":"Reference"},{"location":"reference/#reference","text":"","title":"Reference"},{"location":"segpy/","text":"Segpy The following steps show how to run the segregation pipeline using seg module. segpy workflow Contents Step 1: Run Spark Step 2: Create table matrix Step 3: Run segregation Step 4: Parsing Step 5: Shut down spark Step 1: Run Spark First activate Spark on your system export SPARK_HOME=$HOME/spark-3.1.3-bin-hadoop3.2 export SPARK_LOG_DIR=$HOME/temp module load java/11.0.2 cd ${SPARK_HOME}; ./sbin/start-master.sh Step 2: Create table matrix Next, initialize the hail and import your vcf file and write it as a matrix table, the matrix table is a data structure to present the genetic data as a matrix. In the below, we import the vcf file and write it as MatrixTable , then read your matrix table. For the tutorial, we add data to test the pipeline [https://github.com/The-Neuro-Bioinformatics-Core/segpy/data]. The pipeline is explained using this dataset. The following code imports VCF as a MatrixTable: import sys import pandas as pd import hail as hl hl.import_vcf('~/test/data/VEP.iPSC.vcf',force=True,reference_genome='GRCh38',array_elements_required=False).write('~/test/output/VEP.iPSC.mt', overwrite=True) mt = hl.read_matrix_table('~/test/output/VEP.iPSC.mt') Step 3: Run segregation Run the following codes to generate the segregation. from segpy import seg ped=pd.read_csv('~/test/data/iPSC_2.ped'.ped',sep='\\t') destfolder= '~/test/output/' vcffile='~/test/data/VEP.iPSC.vcf' ncol=7 CSQ=True seg.run(mt,ped,outfolder,hl,ncol,CSQ,vcffile) It generates two files header.txt and finalseg.csv in the destfolder ; header.txt includes the header of information in finalseg.csv . The output of finalseg.csv can be categorized to 1) locus and alleles, 2) CSQ, 3) Global- Non-Affected 4) Global-Affected, 5) Family, 6) Family-Affected 7) Family - Non-affected. If you do not want to have CSQ in the output file, choose CSQ=False . Step 4: Parsing If you want to parse the file, run the following codes, it drops the un-necessary characters \", [, ], etc from the output. from segpy import parser parser.clean_general(outfolder) The info includes several info, to drop the multiplicity run the following code parser.clean_unique(outfolder) Step 5: Shut down spark Do not forget to deactivate environment and stop the spark: cd ${SPARK_HOME}; ./sbin/stop-master.sh Todo \u2b06 back to top","title":"- segpy"},{"location":"segpy/#segpy","text":"The following steps show how to run the segregation pipeline using seg module. segpy workflow","title":"Segpy"},{"location":"segpy/#contents","text":"Step 1: Run Spark Step 2: Create table matrix Step 3: Run segregation Step 4: Parsing Step 5: Shut down spark","title":"Contents"},{"location":"segpy/#step-1-run-spark","text":"First activate Spark on your system export SPARK_HOME=$HOME/spark-3.1.3-bin-hadoop3.2 export SPARK_LOG_DIR=$HOME/temp module load java/11.0.2 cd ${SPARK_HOME}; ./sbin/start-master.sh","title":"Step 1: Run Spark"},{"location":"segpy/#step-2-create-table-matrix","text":"Next, initialize the hail and import your vcf file and write it as a matrix table, the matrix table is a data structure to present the genetic data as a matrix. In the below, we import the vcf file and write it as MatrixTable , then read your matrix table. For the tutorial, we add data to test the pipeline [https://github.com/The-Neuro-Bioinformatics-Core/segpy/data]. The pipeline is explained using this dataset. The following code imports VCF as a MatrixTable: import sys import pandas as pd import hail as hl hl.import_vcf('~/test/data/VEP.iPSC.vcf',force=True,reference_genome='GRCh38',array_elements_required=False).write('~/test/output/VEP.iPSC.mt', overwrite=True) mt = hl.read_matrix_table('~/test/output/VEP.iPSC.mt')","title":"Step 2:  Create table matrix"},{"location":"segpy/#step-3-run-segregation","text":"Run the following codes to generate the segregation. from segpy import seg ped=pd.read_csv('~/test/data/iPSC_2.ped'.ped',sep='\\t') destfolder= '~/test/output/' vcffile='~/test/data/VEP.iPSC.vcf' ncol=7 CSQ=True seg.run(mt,ped,outfolder,hl,ncol,CSQ,vcffile) It generates two files header.txt and finalseg.csv in the destfolder ; header.txt includes the header of information in finalseg.csv . The output of finalseg.csv can be categorized to 1) locus and alleles, 2) CSQ, 3) Global- Non-Affected 4) Global-Affected, 5) Family, 6) Family-Affected 7) Family - Non-affected. If you do not want to have CSQ in the output file, choose CSQ=False .","title":"Step 3: Run segregation"},{"location":"segpy/#step-4-parsing","text":"If you want to parse the file, run the following codes, it drops the un-necessary characters \", [, ], etc from the output. from segpy import parser parser.clean_general(outfolder) The info includes several info, to drop the multiplicity run the following code parser.clean_unique(outfolder)","title":"Step 4: Parsing"},{"location":"segpy/#step-5-shut-down-spark","text":"Do not forget to deactivate environment and stop the spark: cd ${SPARK_HOME}; ./sbin/stop-master.sh","title":"Step 5:  Shut down spark"},{"location":"segpy/#todo","text":"\u2b06 back to top","title":"Todo"},{"location":"segpy_slurm/","text":"Segpy.slurm segpy.slurm is a shield developed to run the segpy pipeline in HPC system (slurm) to submit jobs (step 0 to step 3), it has been used under Beluga , an HPC that uses slurm system. Contents Step 0: Setup Step 1: Create table matrix Step 2: Run segregation Step 3: Clean final data The following followchart show the step of running the segregation analying usning segpy_slurm segpy.svn workflow To run the pipepline, you need 1) path of the pipeline (PIPELINE_HOME), 2) Working directory , 3) VCF, and 4) PED file export PIPELINE_HOME=~/segpy.slurm PWD=~/outfolder VCF=~/data/VEP_iPSC.vcf PED=~/data/iPSC_2.ped Step 0: Setup First run the following code to setup the pipeline, you can change the the parameters in ${PWD}/job_output/segpy.config.ini sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 0 Step 1: Create table matrix The following code, create MatrixTable from the VCF file sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1 \\ --vcf ${VCF} Step 2: Run segregation sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 2 \\ --vcf ${VCF} \\ --ped ${PED} Step 3: Clean final data The following drops the unnecessary characters from the output. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --clean general The following drops the multiplicities info. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --clean unique Note You can easily run step 1 to 3 together, see below sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1-3 \\ --vcf ${VCF} \\ --ped ${PED} \\ --clean general \u2b06 back to top","title":"- segpy.slurm"},{"location":"segpy_slurm/#segpyslurm","text":"segpy.slurm is a shield developed to run the segpy pipeline in HPC system (slurm) to submit jobs (step 0 to step 3), it has been used under Beluga , an HPC that uses slurm system.","title":"Segpy.slurm"},{"location":"segpy_slurm/#contents","text":"Step 0: Setup Step 1: Create table matrix Step 2: Run segregation Step 3: Clean final data The following followchart show the step of running the segregation analying usning segpy_slurm segpy.svn workflow To run the pipepline, you need 1) path of the pipeline (PIPELINE_HOME), 2) Working directory , 3) VCF, and 4) PED file export PIPELINE_HOME=~/segpy.slurm PWD=~/outfolder VCF=~/data/VEP_iPSC.vcf PED=~/data/iPSC_2.ped","title":"Contents"},{"location":"segpy_slurm/#step-0-setup","text":"First run the following code to setup the pipeline, you can change the the parameters in ${PWD}/job_output/segpy.config.ini sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 0","title":"Step 0: Setup"},{"location":"segpy_slurm/#step-1-create-table-matrix","text":"The following code, create MatrixTable from the VCF file sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1 \\ --vcf ${VCF}","title":"Step 1: Create table matrix"},{"location":"segpy_slurm/#step-2-run-segregation","text":"sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 2 \\ --vcf ${VCF} \\ --ped ${PED}","title":"Step 2: Run segregation"},{"location":"segpy_slurm/#step-3-clean-final-data","text":"The following drops the unnecessary characters from the output. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --clean general The following drops the multiplicities info. sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 3 \\ --clean unique","title":"Step 3: Clean final data"},{"location":"segpy_slurm/#note","text":"You can easily run step 1 to 3 together, see below sh $PIPELINE_HOME/launch_pipeline.segpy.sh \\ -d ${PWD} \\ --steps 1-3 \\ --vcf ${VCF} \\ --ped ${PED} \\ --clean general \u2b06 back to top","title":"Note"},{"location":"tutorial/","text":"Tutorial This section contains tutorials for segregation analysis. segpy segpy.slurm","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"This section contains tutorials for segregation analysis.","title":"Tutorial"},{"location":"tutorial/#segpy","text":"","title":"segpy"},{"location":"tutorial/#segpyslurm","text":"","title":"segpy.slurm"}]}